{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7fyfNHcGNiHnYgtq2B/MB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6I5ok_ljDDxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"jTM8Y6qqwAZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","!pip3 install py7zr\n"],"metadata":{"id":"bao6TFmVuobk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Read in all the files from the data file, terrorism data.7z\n","import os\n","import py7zr\n","import tempfile\n","import  nltk\n","import string\n","\n","from gensim.utils import tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","from sklearn.feature_extraction import text\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk import ngrams\n","from collections import Counter\n","\n","def extract_and_read_text_files(archive_path):\n","    text_data = \"\"  # Initialize an empty string to store text\n","\n","    with py7zr.SevenZipFile(archive_path, mode='r') as z:\n","        # Create a temporary directory to extract files\n","        with tempfile.TemporaryDirectory() as tmp_dir:\n","            # Extract all files from the archive to the temporary directory\n","            z.extractall(path=tmp_dir)\n","\n","            # Iterate over all files in the temporary directory\n","            for root, _, files in os.walk(tmp_dir):\n","                for file in files:\n","                    file_path = os.path.join(root, file)\n","                    # Read only text files\n","                    if file_path.endswith('.txt'):\n","                        with open(file_path, 'r') as f:\n","                            # Read and print the contents of the text file\n","                            text_data += f.read()\n","    return text_data\n","\n","def find_bigrams(input_list):\n","  bigram_list = []\n","  for i in range(len(input_list)-1):\n","      bigram_list.append((input_list[i], input_list[i+1]))\n","\n","  return bigram_list\n","\n","# Example usage:\n","archive_path = \"/content/drive/MyDrive/Colab Notebooks/TextMining/terrorism data.7z\"  # Path to your .7z archive\n","all_text = extract_and_read_text_files(archive_path)\n","#print(len(all_text))  # Print the stored text\n","\n","#5. Use the demo code to find bigrams\n","bigrams = find_bigrams(all_text)\n","\n","\n","#7.a Clean the text using the techniques from lectures. You\n","# should remove punctuations, stop words, use stemming or lemmatization etc. Leave the numbers\n","#======================================\n","all_text = all_text.translate(str.maketrans(' ',' ', string.punctuation))\n","\n","stop_words_NLTK = set(stopwords.words('english'))\n","#print(stop_words_NLTK)\n","my_stop_words = stop_words_NLTK\n","#print(len(my_stop_words))\n","\n","stop_words_sklearn = text.ENGLISH_STOP_WORDS\n","#print(stop_words_sklearn)\n","my_stop_words = stop_words_sklearn.union(my_stop_words)\n","#print(len(my_stop_words))\n","\n","stop_words_Spacy = STOP_WORDS\n","#print(stop_words_Spacy)\n","my_stop_words = stop_words_Spacy.union(my_stop_words)\n","#print(my_stop_words)\n","#print(len(my_stop_words))\n","\n","tokens = tokenize(all_text)\n","all_text = [i for i in tokens if not i in stop_words_NLTK]\n","all_text = ' '.join(all_text)\n","print (len(all_text))\n","\n","stemmer= PorterStemmer()\n","text10 = 'The city of Auckalnd\\'s team is Blues but its not confirmed'\n","#print(all_text)\n","text10_tokens=word_tokenize(text10)\n","for word in text10_tokens:\n","    print(stemmer.stem(word))\n","\n","lemmatizer=WordNetLemmatizer()\n","for word in text10_tokens:\n","    print(lemmatizer.lemmatize(word))\n","\n","#print(type(all_text))\n","\n","#7.b Find trigrams of tokens for the all the articles in the dataset.\n","# =========================================\n","text = nltk.word_tokenize(\"The quick brown fox jumped on the lazy dog.\")\n","\n","def find_trigrams(input_list):\n","  bigram_list = []\n","  for i in range(len(input_list)-2):\n","      bigram_list.append((input_list[i], input_list[i+1], input_list[i+2]))\n","\n","  return bigram_list\n","#get individual items from the bigram\n","bigrams = find_trigrams(all_text)\n","#print(bigrams)\n","print(len(bigrams))\n","print(bigrams[2].__getitem__(0))\n","print(bigrams[2].__getitem__(1))\n","print(bigrams[2].__getitem__(2))\n","\n","#Find trigrams of tokens\n","n = 3\n","gramsList = ngrams(all_text.split(), n)\n","ngrams = []\n","for grams in gramsList:\n","  ngrams.append(grams)\n","\n","print(len(ngrams))\n","\n","#7.c Determine the most common trigram\n","# =====================================\n","# Flatten the list of tuples to get individual sequences\n","#flat_sequences = [item for sublist in sequences for item in ngrams]\n","\n","#print(flat_sequences)\n","\n","#sequence_counter = Counter(zip(flat_sequences[:-2], flat_sequences[1:-1], flat_sequences[2:]))\n","#count and sort objects\n","sequence_counter = Counter(ngrams)\n","print(sequence_counter[1])\n","# Get the most common sequence\n","most_common_sequence = sequence_counter.most_common(1)\n","print(\"Most common sequence:\", most_common_sequence)\n","\n","#7.d the “most common topic” in the corpus.\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxzLqgqLBxxn","executionInfo":{"status":"ok","timestamp":1709793889390,"user_tz":-780,"elapsed":34726,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"f3439e27-bef5-44d8-cd10-86c74f89554a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6716536\n","the\n","citi\n","of\n","auckalnd\n","'s\n","team\n","is\n","blue\n","but\n","it\n","not\n","confirm\n","The\n","city\n","of\n","Auckalnd\n","'s\n","team\n","is\n","Blues\n","but\n","it\n","not\n","confirmed\n","6716534\n","I\n","R\n","U\n","922582\n","0\n","Most common sequence: [(('New', 'York', 'Times'), 1905)]\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","sequences = [('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumped'),\n","             ('fox', 'jumped', 'on'), ('jumped', 'on', 'the'), ('on', 'the', 'lazy'),\n","             ('the', 'lazy', 'dog'), ('lazy', 'dog', '.')]\n","\n","# Flatten the list of tuples to get individual sequences\n","flat_sequences = [item for sublist in sequences for item in sublist]\n","\n","#print(flat_sequences)\n","\n","sequence_counter = Counter(zip(flat_sequences[:-2], flat_sequences[1:-1], flat_sequences[2:]))\n","print(sequence_counter)\n","# Get the most common sequence\n","most_common_sequence = sequence_counter.most_common(1)\n","print(\"Most common sequence:\", most_common_sequence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JY4IZFgTBNO8","executionInfo":{"status":"ok","timestamp":1709665055898,"user_tz":-780,"elapsed":345,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"625bc87f-00e0-4348-b9f7-fc368c317e52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({('The', 'quick', 'brown'): 1, ('quick', 'brown', 'quick'): 1, ('brown', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', 'brown'): 1, ('fox', 'brown', 'fox'): 1, ('brown', 'fox', 'jumped'): 1, ('fox', 'jumped', 'fox'): 1, ('jumped', 'fox', 'jumped'): 1, ('fox', 'jumped', 'on'): 1, ('jumped', 'on', 'jumped'): 1, ('on', 'jumped', 'on'): 1, ('jumped', 'on', 'the'): 1, ('on', 'the', 'on'): 1, ('the', 'on', 'the'): 1, ('on', 'the', 'lazy'): 1, ('the', 'lazy', 'the'): 1, ('lazy', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', 'lazy'): 1, ('dog', 'lazy', 'dog'): 1, ('lazy', 'dog', '.'): 1})\n","Most common sequence: [(('The', 'quick', 'brown'), 1)]\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Example text data\n","corpus = [all_text]\n","\n","print(all_text[:100])\n","# Vectorize the text data\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","# Apply LDA for topic modeling\n","lda = LatentDirichletAllocation(n_components=10, random_state=42)\n","lda.fit(X)\n","\n","# Assign topics to documents\n","document_topics = lda.transform(X)\n","topic_assignments = document_topics.argmax(axis=1)\n","\n","# Count the frequency of each topic\n","topic_counter = Counter(topic_assignments)\n","\n","print(topic_counter.most_common)\n","\n","# Determine the most common topic\n","most_common_topic, count = topic_counter.most_common(1)[0]\n","\n","print(\"Most common topic:\", most_common_topic)\n","print(\"Frequency:\", count)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAtKSlFCo17o","executionInfo":{"status":"ok","timestamp":1709794754167,"user_tz":-780,"elapsed":6112,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"3b1fa480-cf47-48b3-a177-f51b6a125d72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BEIRUT Lebanon An Islamic State militant killed mother front post office northern Syrian city Raqqa \n","<bound method Counter.most_common of Counter({3: 1})>\n","Most common topic: 3\n","Frequency: 1\n"]}]}]}