{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nq7bIGs_ShWiTZxymtkMLaEYJPRtPP_q","timestamp":1712306441061}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# NER using NLTK"],"metadata":{"id":"4TkdYW65UEtN"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD4VNnbSalx1","executionInfo":{"status":"ok","timestamp":1712639704734,"user_tz":-720,"elapsed":22894,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"1de92397-5ec9-4125-af2d-a3dd5eac22a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from nltk import ne_chunk, pos_tag,word_tokenize\n","from nltk.tree import Tree\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","def get_continous_chunks(text):\n","  print(\"text= \", text)\n","  chunked = ne_chunk(pos_tag(word_tokenize(text)))\n","  print(\"word_tokenize(text)= \", word_tokenize(text))\n","  print(\"pos_tag(word_tokenize(text))= \", pos_tag(word_tokenize(text)))\n","  print(\"Parsing: ne_chunk(pos_tag(word_tokenize(text)))= \", chunked)\n","  print(\"1============================= \")\n","  prev = None\n","  continous_chuck =[]\n","  current_chunk = []\n","\n","  for i in chunked:\n","    if type(i) == Tree:\n","      print(i,\"===is tree, i.leaves()=\",i.leaves())\n","      current_chunk.append(\" \".join([token for token,pos in i.leaves()]))\n","    elif current_chunk:\n","      named_entity = \" \".join(current_chunk)\n","      if named_entity not in continous_chuck:\n","        continous_chuck.append(named_entity)\n","        current_chunk = []\n","    else:\n","      continue\n","\n","  if continous_chuck:\n","    named_entity = \" \".join(current_chunk)\n","    if named_entity not in continous_chuck:\n","        continous_chuck.append(named_entity)\n","\n","  return continous_chuck\n","\n","\n","txt = \"Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't. Please correct the result.\"\n","\n","print(get_continous_chunks(txt))\n","print(\"2=============================\")\n","\n","print(\"nltk.sent_tokenize(txt)= \", nltk.sent_tokenize(txt))\n","for sent in nltk.sent_tokenize(txt):\n","  print(\"sent= \", sent)\n","  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n","    print(\"chunk= \", chunk)\n","    if hasattr(chunk,'label'):\n","      print(chunk.label(),' '.join(c[0] for c in chunk))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txco55FgLdDw","executionInfo":{"status":"ok","timestamp":1714252004656,"user_tz":-720,"elapsed":340,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"d2c635bc-2f44-420b-8bd1-7fc25ae26abb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["text=  Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't. Please correct the result.\n","word_tokenize(text)=  ['Jacinda', 'Ardern', 'is', 'the', 'Prime', 'Minister', 'of', 'New', 'Zealand', 'but', 'Roenzo', 'is', \"n't\", '.', 'Please', 'correct', 'the', 'result', '.']\n","pos_tag(word_tokenize(text))=  [('Jacinda', 'NNP'), ('Ardern', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('Prime', 'NNP'), ('Minister', 'NNP'), ('of', 'IN'), ('New', 'NNP'), ('Zealand', 'NNP'), ('but', 'CC'), ('Roenzo', 'NNP'), ('is', 'VBZ'), (\"n't\", 'RB'), ('.', '.'), ('Please', 'VB'), ('correct', 'VB'), ('the', 'DT'), ('result', 'NN'), ('.', '.')]\n","Parsing: ne_chunk(pos_tag(word_tokenize(text)))=  (S\n","  (PERSON Jacinda/NNP)\n","  (ORGANIZATION Ardern/NNP)\n","  is/VBZ\n","  the/DT\n","  Prime/NNP\n","  Minister/NNP\n","  of/IN\n","  (GPE New/NNP Zealand/NNP)\n","  but/CC\n","  (PERSON Roenzo/NNP)\n","  is/VBZ\n","  n't/RB\n","  ./.\n","  Please/VB\n","  correct/VB\n","  the/DT\n","  result/NN\n","  ./.)\n","1============================= \n","(PERSON Jacinda/NNP) ===is tree, i.leaves()= [('Jacinda', 'NNP')]\n","(ORGANIZATION Ardern/NNP) ===is tree, i.leaves()= [('Ardern', 'NNP')]\n","(GPE New/NNP Zealand/NNP) ===is tree, i.leaves()= [('New', 'NNP'), ('Zealand', 'NNP')]\n","(PERSON Roenzo/NNP) ===is tree, i.leaves()= [('Roenzo', 'NNP')]\n","['Jacinda Ardern', 'New Zealand', 'Roenzo', '']\n","2=============================\n","nltk.sent_tokenize(txt)=  [\"Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't.\", 'Please correct the result.']\n","sent=  Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't.\n","chunk=  (PERSON Jacinda/NNP)\n","PERSON Jacinda\n","chunk=  (ORGANIZATION Ardern/NNP)\n","ORGANIZATION Ardern\n","chunk=  ('is', 'VBZ')\n","chunk=  ('the', 'DT')\n","chunk=  ('Prime', 'NNP')\n","chunk=  ('Minister', 'NNP')\n","chunk=  ('of', 'IN')\n","chunk=  (GPE New/NNP Zealand/NNP)\n","GPE New Zealand\n","chunk=  ('but', 'CC')\n","chunk=  (PERSON Roenzo/NNP)\n","PERSON Roenzo\n","chunk=  ('is', 'VBZ')\n","chunk=  (\"n't\", 'RB')\n","chunk=  ('.', '.')\n","sent=  Please correct the result.\n","chunk=  ('Please', 'VB')\n","chunk=  ('correct', 'VB')\n","chunk=  ('the', 'DT')\n","chunk=  ('result', 'NN')\n","chunk=  ('.', '.')\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}]},{"cell_type":"code","source":[" import nltk\n","\n"," nltk.download('punkt')\n","\n"," nltk.download('averaged_perceptron_tagger')\n","\n"," nltk.download('maxent_ne_chunker')\n","\n"," nltk.download('words')\n","\n","\n","#DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/TextMining/lab07/sampleText2.txt'\n","\n","\n","with open(DATA_DIR, 'r',encoding='utf-8', errors='ignore') as f:\n","\n","     sample = f.read()\n","\n","sentences = nltk.sent_tokenize(sample)\n","\n","tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","\n","tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n","\n","chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gpHjSmweRsaf","executionInfo":{"status":"ok","timestamp":1712639787939,"user_tz":-720,"elapsed":362,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"2bcea598-1197-47e3-cd25-6d6576c81819"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}]}]}