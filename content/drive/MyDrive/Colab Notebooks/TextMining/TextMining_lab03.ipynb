{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPq30sDLcj2nMmUji3kh5KY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6I5ok_ljDDxh","executionInfo":{"status":"ok","timestamp":1710403876837,"user_tz":-780,"elapsed":23411,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"c9ef5391-9858-4840-8a61-90025ac958c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","#2.Use the tags from end of the lecture 1 to manually tag all the sentences in the paragraph\n","  #use following code instead of manual\n","#3. POS tag all the sentences in the paragraph\n","with open('/content/drive/MyDrive/Colab Notebooks/TextMining/lab03-res/txtMining_lab03_1.txt', 'r') as file:\n","    # Read the entire contents of the file into a string\n","    file_contents = file.read()\n","\n","tokens = nltk. word_tokenize(file_contents)\n","postags = nltk.pos_tag(tokens)\n","print(postags)\n","\n","#5. determine the Precision, Recall and F1 value\n","with open('/content/drive/MyDrive/Colab Notebooks/TextMining/lab03-res/txtMining_lab03_1_manual_tag.txt', 'r') as file:\n","    # Read the entire contents of the file into a string\n","    file_contents_manual_tag = file.read()\n","import ast\n","postags_manual = ast.literal_eval(file_contents_manual_tag)\n","#print(\"postags_manual===========\")\n","#print(postags_manual)\n","#len(postags_manual)\n","\n","#unigram_tagger = nltk.UnigramTagger(postags)\n","#print(unigram_tagger.evaluate(postags_manual))\n","\n","from nltk.metrics import *\n","ref  =   [tag for _, tag in postags_manual]\n","tagged = [tag for _, tag in postags]\n","cm = ConfusionMatrix(ref, tagged)\n","print(cm)\n","print(\"Precision: \",precision(set(ref),set(tagged)))\n","print(\"Recall: \",recall(set(ref),set(tagged)))\n","print(\"F measure: \",f_measure(set(ref),set(tagged)))\n","print(\"Accuracy: \",accuracy(ref,tagged))\n","\n","#6.\n","import spacy\n","from spacy.matcher import Matcher\n","import os\n","\n","# Load SpaCy English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Initialize SpaCy Matcher with patterns\n","matcher = Matcher(nlp.vocab)\n","\n","# Extract all the clauses in which some type of point scoring is reported.\n","#Dalano Banton scored 25 points\n","person_verb = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}, {\"POS\": \"VERB\"}]\n","score = [{'POS': 'NUM'}, {'LOWER': 'points'}]\n","\n","matcher.add('person''score', patterns=[person_verb,score])\n","\n","\n","\n","# Directory containing NBA articles\n","directory = \"/content/drive/MyDrive/Colab Notebooks/TextMining/lab03-res/NBA\"\n","\n","# Iterate over files in the directory\n","# Read the articles from the directory.\n","for filename in os.listdir(directory):\n","    # Read the content of each file\n","    with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n","        text = file.read()\n","\n","    # Process the text using SpaCy\n","    doc = nlp(text)\n","\n","    # Apply the matcher to the processed text\n","    result = matcher(doc,as_spans=True)\n","    print(result)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIlR7TFJoktV","executionInfo":{"status":"ok","timestamp":1710403896978,"user_tz":-780,"elapsed":11295,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"cf2a3783-f88c-4ef2-e512-a5e36722826f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('Trusts', 'NNS'), ('that', 'WDT'), ('earn', 'VBP'), ('less', 'JJR'), ('than', 'IN'), ('$', '$'), ('10,000', 'CD'), ('likely', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('exempt', 'JJ'), ('from', 'IN'), ('tax', 'NN'), ('hike', 'NN'), ('The', 'DT'), ('Government', 'NNP'), ('has', 'VBZ'), ('detailed', 'VBN'), ('how', 'WRB'), ('it', 'PRP'), ('wants', 'VBZ'), ('to', 'TO'), ('exempt', 'VB'), ('low-income', 'JJ'), ('earning', 'VBG'), ('trusts', 'NNS'), ('from', 'IN'), ('a', 'DT'), ('tax', 'NN'), ('hike', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('take', 'VB'), ('effect', 'NN'), ('on', 'IN'), ('April', 'NNP'), ('1', 'CD'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('proposing', 'VBG'), ('to', 'TO'), ('ensure', 'VB'), ('trusts', 'NNS'), ('that', 'WDT'), ('earn', 'VBP'), ('less', 'JJR'), ('than', 'IN'), ('$', '$'), ('10,000', 'CD'), ('a', 'DT'), ('year', 'NN'), ('continue', 'NN'), ('to', 'TO'), ('be', 'VB'), ('taxed', 'VBN'), ('at', 'IN'), ('33', 'CD'), ('per', 'IN'), ('cent', 'NN'), (',', ','), ('leaving', 'VBG'), ('those', 'DT'), ('that', 'WDT'), ('earn', 'VBP'), ('more', 'JJR'), ('than', 'IN'), ('this', 'DT'), ('sum', 'NN'), ('to', 'TO'), ('be', 'VB'), ('taxed', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('trustee', 'NN'), ('rate', 'NN'), ('of', 'IN'), ('39', 'CD'), ('per', 'IN'), ('cent', 'NN'), ('on', 'IN'), ('all', 'DT'), ('their', 'PRP$'), ('income', 'NN'), ('.', '.'), ('Last', 'JJ'), ('year', 'NN'), (',', ','), ('the', 'DT'), ('Labour', 'JJ'), ('Government', 'NNP'), ('decided', 'VBD'), ('to', 'TO'), ('lift', 'VB'), ('the', 'DT'), ('trustee', 'NN'), ('tax', 'NN'), ('rate', 'NN'), ('from', 'IN'), ('33', 'CD'), ('to', 'TO'), ('39', 'CD'), ('per', 'IN'), ('cent', 'NN'), ('from', 'IN'), ('the', 'DT'), ('2024-25', 'JJ'), ('tax', 'NN'), ('year', 'NN'), ('.', '.'), ('It', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('ensure', 'VB'), ('the', 'DT'), ('trustee', 'NN'), ('rate', 'NN'), ('aligned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('relatively', 'RB'), ('new', 'JJ'), ('top', 'JJ'), ('income', 'NN'), ('tax', 'NN'), ('rate', 'NN'), ('of', 'IN'), ('39', 'CD'), ('per', 'IN'), ('cent', 'NN'), (',', ','), ('applied', 'VBD'), ('to', 'TO'), ('income', 'NN'), ('over', 'IN'), ('$', '$'), ('180,000', 'CD'), ('.', '.'), ('Labour', 'NN'), ('was', 'VBD'), ('worried', 'VBN'), ('that', 'IN'), ('if', 'IN'), ('the', 'DT'), ('two', 'CD'), ('rates', 'NNS'), ('didn', 'VBP'), ('â€™', 'JJ'), ('t', 'NN'), ('align', 'NN'), (',', ','), ('people', 'NNS'), ('would', 'MD'), ('use', 'VB'), ('trusts', 'NNS'), ('to', 'TO'), ('minimise', 'VB'), ('their', 'PRP$'), ('income', 'NN'), ('tax', 'NN'), ('bills', 'NNS'), ('.', '.')]\n","     |                                         P                               |\n","     |                       J        N  N  P  R           V  V  V  V  V  W  W |\n","     |           C  D  I  J  J  M  N  N  N  R  P  R  T  V  B  B  B  B  B  D  R |\n","     |  $  ,  .  D  T  N  J  R  D  N  P  S  P  $  B  O  B  D  G  N  P  Z  T  B |\n","-----+-------------------------------------------------------------------------+\n","   $ | <3> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","   , |  . <4> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","   . |  .  . <5> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","  CD |  .  .  .<10> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","  DT |  .  .  .  .<13> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","  IN |  .  .  .  .  .<21> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","  JJ |  .  .  .  .  .  .<11> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n"," JJR |  .  .  .  .  .  .  . <3> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","  MD |  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n","  NN |  .  .  .  .  .  .  .  .  .<32> .  .  .  .  .  .  .  .  .  .  .  .  .  . |\n"," NNP |  .  .  .  .  .  .  .  .  .  . <3> .  .  .  .  .  .  .  .  .  .  .  .  . |\n"," NNS |  .  .  .  .  .  .  .  .  .  .  . <7> .  .  .  .  .  .  .  .  .  .  .  . |\n"," PRP |  .  .  .  .  .  .  .  .  .  .  .  . <3> .  .  .  .  .  .  .  .  .  .  . |\n","PRP$ |  .  .  .  .  .  .  .  .  .  .  .  .  . <2> .  .  .  .  .  .  .  .  .  . |\n","  RB |  .  .  .  .  .  .  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  . |\n","  TO |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .<11> .  .  .  .  .  .  .  . |\n","  VB |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .<10> .  .  .  .  .  .  . |\n"," VBD |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <4> .  .  .  .  .  . |\n"," VBG |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <3> .  .  .  .  . |\n"," VBN |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <5> .  .  .  . |\n"," VBP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <4> .  .  . |\n"," VBZ |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <3> .  . |\n"," WDT |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <3> . |\n"," WRB |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <1>|\n","-----+-------------------------------------------------------------------------+\n","(row = reference; col = test)\n","\n","Precision:  1.0\n","Recall:  1.0\n","F measure:  1.0\n","Accuracy:  1.0\n","[Boston Celtics look, Jayson Tatum said, 29 points, Jaylen Brown added, 27 points, 14 points, 10 points, Joe Mazzulla said, Trail Blazers enter, Deandre Ayton returned, 30 points, Chauncey Billups said, 13 points, Dalano Banton scored, 25 points, Anfernee Simons added, 23 points, Trail Blazers led, 21 points]\n","[Cade Cunningham scored, 22 points, 1,200 points, Isiah Thomas did, 20 points, Steve Clifford said, 19 points, Miles Bridges had, 24 points, Grant Williams made, 22 points, Simone Fontecchio scored, 17 points, 12 points, 11.6 points, two points, Monty Williams said, James Wiseman helped, 10 points, Golden State drafting, 99 points]\n","[Kevin Durant scored, 37 points, Devin Booker had, Bradley Beal had, 24 points, Grayson Allen scored, Jusuf Nurkic added, nine points, Frank Vogel said, Darius Garland scored, 30 points, Caris LeVert had, 17 points, Isaac Okoro committed, 21 points, 28.0 points, J.B. Bickerstaff agreed]\n","[Luka Doncic had, 27 points, Dallas Mavericks rolled, Daniel Gafford made, 20 points, Lively II scored, Onuralp Bitim led, 17 points, 15 points, Josh Green nailed]\n","[Denver Nuggets avoided, 44 points, 44 points, Michael Malone said, Darko Rajakovic said, 20.1 points, Christian Braun making, Reggie Jackson working]\n"]}]},{"cell_type":"markdown","source":["**Lecture Sample Code**"],"metadata":{"id":"e6NykfEsxJNc"}},{"cell_type":"code","source":["import nltk\n","from pprint import pprint\n","nltk.download('brown')\n","from nltk.corpus import brown\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","brown_sents = brown.sents(categories='news')\n","brown_tagged_sents\n","#type(brown_tagged_sents)\n","len(brown_tagged_sents)\n","print(type(brown_tagged_sents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vuoregIyemU","executionInfo":{"status":"ok","timestamp":1710215835705,"user_tz":-780,"elapsed":675,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"ad89ad1c-7fd7-4d52-d385-39cb9070aa3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n"]}]},{"cell_type":"code","source":["import nltk\n","from pprint import pprint\n","nltk.download('brown')\n","from nltk.corpus import brown\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","brown_sents = brown.sents(categories='news')\n","patterns = [\n","     (r'.*ing$', 'VBG'),               # gerunds\n","     (r'.*ed$', 'VBD'),                # simple past\n","     (r'.*es$', 'VBZ'),                # 3rd singular present\n","     (r'.*ould$', 'MD'),               # modals\n","     (r'.*\\'s$', 'NN$'),               # possessive nouns\n","     (r'.*s$', 'NNS'),                 # plural nouns\n","     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n","     (r'.*', 'NN')                     # nouns (default)\n"," ]\n","regexp_tagger = nltk.RegexpTagger(patterns)\n","print(\"sssssss\")\n","print(brown_sents[3])\n","print(\"sssssss2\")\n","#print(regexp_tagger.tag(brown_sents[3]))\n","#print(\"Accuracy: \",  regexp_tagger.evaluate((brown_tagged_sents)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTM8Y6qqwAZC","executionInfo":{"status":"ok","timestamp":1710212738367,"user_tz":-780,"elapsed":333,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"a5659c83-9312-421f-b2e9-6bc90cbd0009"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sssssss\n","['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n","sssssss2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"]}]},{"cell_type":"code","source":["from spacy import displacy\n","\n","from spacy.symbols import nsubj, VERB, dobj, pobj, NOUN,PROPN, PERSON\n","\n","import spacy\n","\n","# Load the language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","doc = nlp(\"The bus drove into the pole with speed\")\n","\n","\n","\n","for token in doc:\n","\n","\n","\n","\n","    if token.dep == nsubj and token.head.pos == VERB:\n","      print(token)\n","      print([child for child in token.head.children]) # Tree navigation\n","\n","    if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n","        print(\"Main verb:\", token.text)\n","        print([child for child in token.children]) # Tree navigation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4fEVIVlreYC","executionInfo":{"status":"ok","timestamp":1710403773803,"user_tz":-780,"elapsed":823,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"c9209c12-48d8-4cd2-cfec-dd4776d64c85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bus\n","[bus, into, with]\n","Main verb: drove\n","[bus, into, with]\n"]}]},{"cell_type":"code","source":["\n","def printDocTags(doc):\n","\n","  for token in doc:\n","\n","    print(token.text,'/', token.pos_)\n","\n","\n","\n","\n","\n","from spacy.matcher import Matcher\n","\n","doc = nlp('Johnny Jumped in the river')\n","#doc = nlp('Johnny jumped river')\n","\n","\n","#printDocDep(doc)\n","\n","printDocTags(doc)\n","\n","matcher = Matcher(vocab = nlp.vocab)\n","\n","#clause= [{'POS': 'VERB'},{'POS': 'NOUN'}]\n","#clause= [{'POS': 'PROPN'},{'POS': 'VERB'},{'POS': 'NOUN'}]\n","clause= [{'POS': 'PROPN'},{'POS': 'PROPN'},{'POS': 'ADP'},{'POS': 'DET'},{'POS': 'NOUN'}]\n","p_v = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}, {\"POS\": \"VERB\"}]\n","\n","\n","\n","matcher.add('clause', patterns=[clause])\n","\n","\n","\n","result = matcher(doc,as_spans=True)\n","\n","print(result)\n","\n","\n","\n","def printDocDep(doc):\n","  for token in doc:\n","    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n","\n","\n","doc = nlp(\"I went to town on a bicycle\")\n","\n","printDocDep(doc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pmgd3quZDhWn","executionInfo":{"status":"ok","timestamp":1710408772166,"user_tz":-780,"elapsed":529,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"fdfb4e40-9df4-4ecb-ae7f-c18e330cbe8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Johnny / PROPN\n","Jumped / PROPN\n","in / ADP\n","the / DET\n","river / NOUN\n","[Johnny Jumped in the river]\n"," /_SP <--dep--  /_SP\n","I/PRP <--nsubj-- went/VBD\n","went/VBD <--ROOT-- went/VBD\n","to/IN <--prep-- went/VBD\n","town/NN <--pobj-- to/IN\n","on/IN <--prep-- went/VBD\n","a/DT <--det-- bicycle/NN\n","bicycle/NN <--pobj-- on/IN\n"]}]},{"cell_type":"code","source":["doc = nlp(\"COMP700 Quiz held in Auckland, New Zealand\")\n","\n","for chunk in doc.noun_chunks:\n","\n","    print(chunk.text, chunk.label_, '[', chunk.root.text, ']')"],"metadata":{"id":"SOiVhDO8aPKX","executionInfo":{"status":"ok","timestamp":1710410497689,"user_tz":-780,"elapsed":344,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"683ae2e2-0323-4ea8-d90d-cc23cb94c0ba","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Quiz NP [ Quiz ]\n","Auckland NP [ Auckland ]\n","New Zealand NP [ Zealand ]\n"]}]}]}