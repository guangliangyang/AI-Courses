{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18R5V2lN9fbatJVrToAD3Xcjw12OXLtU_","timestamp":1714332278064},{"file_id":"1nq7bIGs_ShWiTZxymtkMLaEYJPRtPP_q","timestamp":1712306441061}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD4VNnbSalx1","executionInfo":{"status":"ok","timestamp":1714444071030,"user_tz":-720,"elapsed":3275,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"488eb356-788a-4110-b0d4-25f9d36fb57d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# **Session 2 Example code**"],"metadata":{"id":"2jXionXcLrus"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","\n","text1 = \"This is an exmaple of pre-processing\"\n","text = nltk.word_tokenize(text1)\n","\n","#Use of a different tokenizer\n","from gensim.utils import tokenize # Note: this library is not part of nltk so install using \"pip3 install gensim\" if required.\n","print(list(tokenize(text1)))\n","\n","#remove numbers\n","import re\n","text1 = re.sub(r'\\d+','',text1)\n","\n","#remove punctuation\n","import string\n","text1 = text1.translate(str.maketrans(' ',' ', string.punctuation))\n","\n","#remove white space\n","text1 = text1.strip()\n","\n","##==================remove stop words, nltk\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","stop_words_NLTK = set(stopwords.words('english'))\n","tokens = tokenize(text1)\n","result = [i for i in tokens if not i in stop_words_NLTK]\n","#print (result)\n","my_stop_words = stop_words_NLTK\n","#print(len(my_stop_words))\n","\n","##==================remove stop words 2 , other Lib, sklearn\n","from sklearn.feature_extraction import text\n","stop_words_sklearn = text.ENGLISH_STOP_WORDS\n","print(stop_words_sklearn)\n","my_stop_words = stop_words_sklearn.union(my_stop_words) #累加stop_words\n","print(len(my_stop_words))\n","\n","##==================remove stop words 3 , other Lib, spacy\n","from spacy.lang.en.stop_words  import STOP_WORDS\n","stop_words_Spacy = STOP_WORDS\n","print(stop_words_Spacy)\n","my_stop_words = stop_words_Spacy.union(my_stop_words)  #累加stop_words\n","\n","##==================Stemming (先tokenize, 后stemming)\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","stemmer= PorterStemmer()\n","text10 = 'The city of Auckalnd\\'s team is Blues but its not confirmed'\n","text10_tokens=word_tokenize(text10)\n","for word in text10_tokens:\n","    print(stemmer.stem(word))\n","\n","##==================Lemmatization(先tokenize, 后Lemmatization; better than steamming)\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","lemmatizer=WordNetLemmatizer()\n","for word in text10_tokens:\n","    print(lemmatizer.lemmatize(word))\n","\n","##==================customer bi-grams\n","import  nltk\n","\n","text = nltk.word_tokenize(\"The quick brown fox jumped on the lazy dog.\")\n","def find_bigrams(input_list):\n","  bigram_list = []\n","  for i in range(len(input_list)-1):\n","      bigram_list.append((input_list[i], input_list[i+1]))\n","\n","  return bigram_list\n","#get individual items from the bigram\n","bigrams = find_bigrams(text)\n","\n","##==================n-gram function from NLTK\n","from nltk import ngrams\n","sentence = 'The quick brown fox jumped over the lazy dog.'\n","n = 3\n","gramsList = ngrams(sentence.split(), n)\n","ngrams = []\n","for grams in gramsList:\n","  ngrams.append(grams)\n","print(ngrams)\n","\n","##=============read files\n","import os\n","from pprint import pprint\n","\n","with os.scandir('/content/drive/MyDrive/Colab Notebooks/teaching/data/terrorism2') as entries:\n","     lines_in_file = \"\"\n","     for entry in entries:\n","         print(entry.name)\n","         with open(entry, 'r') as file:\n","             lines_in_file = lines_in_file + file.read().lower()\n","#print(lines_in_file)\n","\n","##=============Function to remove stopwords\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","def remove_stop_words(text):\n","    from nltk.corpus import stopwords\n","    from nltk.tokenize import word_tokenize\n","    stop_words = stopwords.words('english')\n","    newStopWords = ['it', 'its', 'when',',',':',';'] # add your own stop words to the list here.\n","    stop_words = stop_words + newStopWords\n","    word_tokens = word_tokenize(text)\n","\n","    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n","    return filtered_sentence\n","\n","##=============Function to find ngrams and return it as an array.\n","def find_ngrams(n, textList):\n","    from nltk import ngrams\n","    gramsList = ngrams(textList, n)\n","    ngrams = []\n","    for grams in gramsList:\n","        ngrams.append(grams)\n","    return ngrams\n","##=============  count the grams and sort them.\n","import collections\n","cleanedLines = remove_stop_words(lines_in_file)\n","grams = find_ngrams(3,cleanedLines)\n","count_grams = collections.Counter(grams)\n","#count and sort\n","most_common_gram = count_grams.most_common(2)\n","print(most_common_gram)\n","#most common\n","print (\"Most common\")\n","print(most_common_gram[0])\n","print(most_common_gram[0].__getitem__(0)) #get the trigram.\n","print(most_common_gram[0].__getitem__(1)) #get the count, the second item in the array.\n","\n","print (\"Second most common\")\n","print(most_common_gram[1])\n","print(most_common_gram[1].__getitem__(0).__getitem__(2)) # get the last item of the trigram, which is the first item in the array.\n","print(\"\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmVbWu7wL9Vi","executionInfo":{"status":"ok","timestamp":1714334056838,"user_tz":-720,"elapsed":8704,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"3dc667c8-a2d7-499a-c725-60cb4870206b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["['This', 'is', 'an', 'exmaple', 'of', 'pre', 'processing']\n","frozenset({'yours', 'often', 'above', 'she', 'an', 'your', 'ours', 'out', 'get', 'into', 'eleven', 'perhaps', 'the', 'take', 'find', 'much', 'nor', 'thence', 'wherever', 'never', 'him', 'together', 'through', 'due', 'detail', 'became', 'up', 'put', 'nobody', 'whereas', 'in', 'this', 'hereby', 'hereafter', 'done', 'some', 'under', 'cant', 'might', 'should', 'five', 'thin', 'before', 'during', 'until', 'otherwise', 'noone', 'sincere', 'three', 'along', 'can', 'anyway', 'hence', 'per', 'almost', 'more', 'everyone', 'not', 'last', 'top', 'you', 'was', 'cannot', 'moreover', 'else', 'except', 'amongst', 'at', 'whatever', 'its', 'if', 'etc', 'therefore', 'six', 'have', 'being', 'ie', 'amount', 'neither', 'somehow', 'everywhere', 'thus', 'sometime', 'one', 'several', 'when', 'his', 'fifty', 'fire', 'enough', 'even', 'who', 'whither', 'others', 'serious', 'all', 'move', 'part', 'do', 'these', 'nothing', 'where', 'found', 'has', 'am', 'anyhow', 'from', 'anyone', 'alone', 'upon', 'that', 'front', 'less', 'mill', 'mine', 'show', 'ltd', 'yet', 'something', 'how', 'itself', 'inc', 'meanwhile', 'amoungst', 'each', 'nine', 'there', 'nevertheless', 'across', 'once', 'against', 'therein', 'were', 'whereupon', 'becomes', 'thereupon', 'is', 'as', 'everything', 'however', 'those', 'whereby', 'sometimes', 'whom', 'they', 'ourselves', 'himself', 'seeming', 'are', 'fifteen', 'also', 'what', 'mostly', 'between', 'without', 'via', 'only', 'ten', 'becoming', 'own', 'her', 'namely', 'over', 'me', 'thick', 'below', 'could', 'had', 'and', 'because', 'herein', 'eg', 'side', 'hers', 'every', 'would', 'such', 'con', 'four', 'my', 'other', 'full', 'twelve', 'hereupon', 'system', 'few', 'bottom', 'us', 'seems', 'always', 'latterly', 'them', 'here', 'thereafter', 'go', 'seemed', 'very', 'but', 'whoever', 'rather', 'whether', 'name', 'ever', 'with', 'whose', 'then', 'beforehand', 'further', 'indeed', 'may', 'fill', 'throughout', 'to', 'about', 'still', 'two', 'now', 'forty', 'none', 'why', 'yourselves', 'least', 'interest', 'been', 'couldnt', 'cry', 'latter', 'off', 'first', 'another', 'it', 'beyond', 'same', 'see', 'back', 'we', 'beside', 'again', 'among', 'anything', 'within', 'besides', 'their', 'third', 'no', 'by', 'twenty', 'though', 'be', 'onto', 'eight', 'yourself', 'or', 'after', 'formerly', 'call', 'many', 'i', 'both', 'since', 'most', 'toward', 'he', 'former', 'thru', 'thereby', 'un', 'down', 'must', 'wherein', 'whenever', 'elsewhere', 'our', 'made', 'already', 'herself', 'which', 'whereafter', 'a', 'so', 'either', 'nowhere', 'afterwards', 'well', 'for', 'describe', 'any', 'on', 'myself', 'co', 're', 'someone', 'hundred', 'anywhere', 'next', 'empty', 'de', 'whole', 'around', 'too', 'seem', 'become', 'give', 'whence', 'behind', 'while', 'keep', 'towards', 'sixty', 'although', 'than', 'themselves', 'hasnt', 'please', 'somewhere', 'will', 'of', 'bill'})\n","378\n","{'yours', 'often', 'above', 'she', 'unless', 'an', 'your', \"'ll\", 'ours', 'say', 'out', 'get', 'into', 'eleven', 'perhaps', 'the', 'take', 'much', 'nor', 'thence', 'wherever', 'never', '’m', 'him', 'together', 'through', 'due', 'became', 'up', 'put', 'nobody', 'whereas', 'in', '’ve', 'this', \"'s\", 'hereby', '’re', 'hereafter', 'done', 'some', 'under', 'might', 'should', 'five', \"'d\", 'before', 'during', 'used', 'really', 'until', 'otherwise', 'noone', '’s', 'three', 'along', 'can', 'quite', 'anyway', 'hence', 'per', 'almost', 'more', 'everyone', 'not', 'last', 'top', 'you', 'was', 'cannot', 'just', 'moreover', 'else', 'except', 'amongst', 'at', 'whatever', 'its', 'if', 'therefore', 'six', 'have', 'being', 'amount', 'neither', 'somehow', 'ca', 'everywhere', 'thus', 'sometime', 'several', 'one', 'when', 'his', 'fifty', '‘d', 'enough', 'even', 'who', 'whither', 'others', 'serious', 'all', 'move', 'part', 'do', 'these', 'nothing', 'where', 'has', 'am', 'anyhow', 'from', 'anyone', 'alone', 'upon', 'that', '‘s', 'front', '‘ll', 'less', 'mine', 'show', 'n‘t', 'yet', 'something', 'how', 'itself', 'meanwhile', 'each', 'nine', 'there', 'nevertheless', 'across', 'once', 'against', 'therein', 'were', 'whereupon', 'becomes', 'thereupon', 'is', 'as', 'everything', 'however', 'those', 'whereby', 'sometimes', 'whom', 'they', 'ourselves', 'himself', 'seeming', 'are', 'fifteen', 'regarding', 'also', 'what', 'mostly', 'between', 'without', 'via', 'only', 'ten', 'becoming', 'own', 'her', 'namely', 'over', 'me', 'below', 'could', 'had', 'and', 'because', 'herein', 'side', 'hers', 'every', 'would', 'n’t', 'such', 'four', 'my', 'other', 'using', 'full', 'twelve', 'hereupon', 'few', 'bottom', 'us', 'seems', 'always', 'latterly', 'them', 'did', 'here', \"'re\", 'thereafter', 'go', 'seemed', 'very', 'but', 'whoever', 'rather', 'whether', 'name', 'ever', 'with', 'various', 'whose', 'then', 'does', 'beforehand', 'further', 'indeed', 'may', 'to', 'throughout', 'about', 'still', 'two', 'now', 'make', 'forty', 'none', 'why', 'yourselves', 'least', 'been', 'latter', 'off', 'first', 'another', 'it', 'beyond', 'same', 'see', 'back', 'we', 'beside', 'again', 'among', 'anything', 'within', '’d', 'besides', 'their', 'third', 'no', 'by', 'doing', 'twenty', 'though', '‘m', 'be', 'onto', 'eight', 'yourself', 'or', 'after', 'formerly', 'call', 'many', 'i', 'both', 'since', 'most', 'toward', \"'m\", 'he', 'former', 'thru', '‘re', 'thereby', 'down', 'must', 'wherein', 'whenever', 'elsewhere', 'our', 'made', 'already', 'herself', 'which', 'whereafter', \"'ve\", 'a', 'so', 'either', 'nowhere', 'afterwards', 'well', 'for', 'any', '’ll', 'on', 'myself', 're', 'someone', \"n't\", '‘ve', 'hundred', 'anywhere', 'next', 'empty', 'whole', 'around', 'too', 'seem', 'become', 'give', 'whence', 'behind', 'while', 'keep', 'towards', 'sixty', 'although', 'than', 'themselves', 'please', 'somewhere', 'will', 'of'}\n","the\n","citi\n","of\n","auckalnd\n","'s\n","team\n","is\n","blue\n","but\n","it\n","not\n","confirm\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["The\n","city\n","of\n","Auckalnd\n","'s\n","team\n","is\n","Blues\n","but\n","it\n","not\n","confirmed\n","[('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumped'), ('fox', 'jumped', 'over'), ('jumped', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog.')]\n"]}]},{"cell_type":"markdown","source":["# **Session 3 Example code**"],"metadata":{"id":"bq6gUTOaRjcE"}},{"cell_type":"code","source":["##=============Regualar expression tagger，  accuracy=0.2\n","import nltk\n","from pprint import pprint\n","nltk.download('brown')\n","from nltk.corpus import brown\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","brown_sents = brown.sents(categories='news')\n","patterns = [\n","     (r'.*ing$', 'VBG'),               # gerunds\n","     (r'.*ed$', 'VBD'),                # simple past\n","     (r'.*es$', 'VBZ'),                # 3rd singular present\n","     (r'.*ould$', 'MD'),               # modals\n","     (r'.*\\'s$', 'NN$'),               # possessive nouns\n","     (r'.*s$', 'NNS'),                 # plural nouns\n","     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n","     (r'.*', 'NN')                     # nouns (default)\n"," ]\n","regexp_tagger = nltk.RegexpTagger(patterns)\n","print(brown_sents[3])\n","pprint(regexp_tagger.tag(brown_sents[3]))\n","print(\"Accuracy: \",  regexp_tagger.evaluate((brown_tagged_sents)))\n","\n","##============= nltk.UnigramTagger ，  accuracy=0.8, Separating training and test data\n","import nltk\n","from nltk.corpus import brown\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","size = int(len(brown_tagged_sents) * 0.9)\n","print(size)\n","train_sents = brown_tagged_sents[:size]\n","test_sents = brown_tagged_sents[size:]\n","unigram_tagger = nltk.UnigramTagger(train_sents)  #training\n","print(unigram_tagger.evaluate(test_sents))        #test\n","\n","\n","#Storing a trained model, retrieving it and using it. 保存模型，加载模型，使用模型。\n","#Store it.\n","from pickle import dump\n","output = open('/content/drive/MyDrive/Colab Notebooks/teaching/models/ugTagger.pkl', 'wb')\n","dump(unigram_tagger, output, -1)\n","output.close()\n","\n","#Retrieve it from a file\n","from pickle import load\n","input = open('/content/drive/MyDrive/Colab Notebooks/teaching/models/ugTagger.pkl', 'rb')\n","tagger = load(input)\n","input.close()\n","#Use it.\n","text = \"The board's action shows what free enterprise is up against in our complex maze of regulatory laws .\"\n","tokens = text.split()\n","print(tagger.tag(tokens))\n","##=============Computing recall and precision using sklearn library. sklearn计算召回率和精准率\n","ref  = 'DET NN VB DET JJ NN NN IN DET NN'.split()\n","tagged = 'DET VB VB DET NN NN NN IN DET NN'.split()\n","from sklearn import metrics\n","print(metrics.classification_report(ref, tagged))\n","##=============Extracting noun phrases/chunking 提取名词短语\n","#Collect all nouns and their modifiers\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n","for chunk in doc.noun_chunks:\n","    print(chunk.text, chunk.label_, chunk.root.text)\n","\n","def printDocDep(doc):\n","  for token in doc:\n","    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n","\n","\n","#标注后，提取单词。 extracting clauses using tagged text; 使用了token.pos,ent_type,dep\n","from spacy.symbols import nsubj, VERB, dobj, pobj, NOUN,PROPN, PERSON\n","\n","persons, nouns,verbs = [],[],[]\n","subjs, dobjs, headVerbs = [],[],[]\n","\n","for token in doc:\n","\n","    if token.pos == NOUN or token.pos == PROPN: nouns.append(token)\n","    if token.pos == VERB: verbs.append(token)\n","    if token.ent_type == PERSON: persons.append(token)\n","\n","    if token.dep == nsubj: subjs.append(token)\n","    if token.dep == dobj: dobjs.append(token)\n","\n","    if token.dep == nsubj and token.head.pos == VERB:\n","        headVerbs.append(token.head)\n","\n","    if token.dep == nsubj and token.head.pos == VERB:\n","      print([child for child in token.head.children]) # Tree navigation\n","print(subjs, headVerbs, dobjs)\n","\n","##=============Display a dependency tree.\n","from spacy import displacy\n","displacy.render(doc, style='dep', jupyter = True, options={'distance': 100})\n","##=============Matching patterns using tags from Spacy 高级库，直接匹配查找\n","from spacy.matcher import Matcher\n","doc = nlp('I scored a try and he talked about it')\n","matcher = Matcher(vocab = nlp.vocab)\n","p_v= [{'POS': 'PRON'},{'POS': 'VERB'}] #pronoun followed by a verb\n","about = [{'POS': 'VERB'},{'LEMMA':'about'}] #verb followd by the token \"about\"\n","matcher.add('pronoun+verb''about', patterns=[p_v,about])\n","\n","result = matcher(doc,as_spans=True)\n","print(result) #[I scored, he talked, talked about]\n"],"metadata":{"id":"DnTZgfynRnAU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Session 4 Example code TFIDF**"],"metadata":{"id":"q6tnmyLIfPby"}},{"cell_type":"code","source":["##=============Convert words to vectors using Count Vectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\"The quick brown fox jumped over the lazy dog.\",\"The quick brown fox jumped.\",\"The fox\"]\n","# create the transform\n","vectorizer = CountVectorizer()\n","#tokenize and build vocab\n","vectorizer.fit(corpus)\n","# encode document\n","vector1 = vectorizer.transform(corpus)\n","##============= sklearn 类库TFID\n","import pandas as pd\n","import numpy as np\n","\n","words_set = set()\n","\n","for doc in  corpus:\n","    words = doc.split(' ')\n","    words_set = words_set.union(set(words))\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tr_idf_model  = TfidfVectorizer()\n","tf_idf_vector = tr_idf_model.fit_transform(corpus)\n","print(type(tf_idf_vector), tf_idf_vector.shape)\n","\n","tf_idf_array = tf_idf_vector.toarray()\n","words_set = tr_idf_model.get_feature_names_out()\n","\n","df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set) #3个文章中，各个词的向量\n","print(df_tf_idf)\n","##=============Distance metrices 文章相似度，测距， nltk.metrics\n","from nltk.metrics import *\n","print(corpus[0])\n","print(corpus[1])\n","print(\"Edit Distnance same string: \",edit_distance(corpus[0],corpus[0]))\n","print(\"Edit Distnance: \",edit_distance(corpus[0],corpus[1]))\n","print(\"Binary Distnance: \",binary_distance(set(corpus[0]),set(corpus[1])))\n","print(\"Jaccard Distnance: \",jaccard_distance(set(corpus[0]),set(corpus[1])))\n","print(\"Masi Distnance: \",masi_distance(set(corpus[0]),set(corpus[1])))\n","\n","from sklearn.metrics.pairwise import euclidean_distances\n","print(\"Euclidean Distnance: \",euclidean_distances(vector1,vector2))\n","##============="],"metadata":{"id":"B39zBv18fUIn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Session 5 Example code -Embedding**"],"metadata":{"id":"vvMOqWWOfnqW"}},{"cell_type":"code","source":["##=============用CNN 实现了 word2vec模型.  求input 和 label 之间的(w1,b1)，输出（ input, w1,b1) 矩阵\n","corpus = ['king is a strong man',\n","          'queen is a wise woman',\n","          'boy is a young man',\n","          'girl is a young woman',\n","          'prince is a young king',\n","          'princess is a young queen',\n","          'man is strong',\n","          'woman is pretty',\n","          'prince is a boy will be king',\n","          'princess is a girl will be queen']\n","\n","def remove_stop_words(corpus) :\n","    stop_words = ['is', 'a', 'will', 'be']\n","    results = []\n","\n","    for text in corpus :\n","        tmp = text.split(' ')\n","        for stop_word in stop_words :\n","            if stop_word in tmp :\n","                tmp.remove(stop_word)\n","        results.append(\" \".join(tmp))\n","\n","    return results\n","\n","corpus = remove_stop_words(corpus)\n","\n","words = []\n","for text in corpus :\n","    for word in text.split(' ') :\n","        words.append(word)\n","\n","words = set(words) #单词集合\n","word2int = {}\n","\n","for i,word in enumerate(words) :#单词编号\n","    word2int[word] = i\n","\n","sentences = []\n","for sentence in corpus :\n","    sentences.append(sentence.split())\n","\n","WINDOW_SIZE = 2\n","\n","data = [] #skipgram 2,\n","for sentence in sentences :\n","    for idx,word in enumerate(sentence) :\n","        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] :\n","            if neighbor != word :\n","                data.append([word, neighbor])\n","\n","\n","import pandas as pd\n","df = pd.DataFrame(data, columns = ['input', 'label']) #input, Label 对应起来，准备训练\n","\n","##============= Tensorflow 训练， 定义结构\n","import tensorflow.compat.v1 as tf # use legacly tensor flow\n","\n","# disabling eager mode\n","tf.compat.v1.disable_eager_execution()\n","import numpy as np\n","\n","ONE_HOT_DIM = len(words) #词汇表的大小（即单词的总数）。\n","\n","# one-hot-encoding\n","def to_one_hot_encoding(data_point_index) :\n","    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n","    one_hot_encoding[data_point_index] = 1\n","\n","    return one_hot_encoding\n","\n","X = [] # input word\n","Y = [] # target word\n","\n","for x, y in zip(df['input'], df['label']) :  # 数据放入one_hot 数组\n","    X.append(to_one_hot_encoding(word2int[ x ]))\n","    Y.append(to_one_hot_encoding(word2int[ y ]))\n","\n","# numpy array\n","X_train = np.asarray(X) #input [1,2,3,5,8,...]\n","Y_train = np.asarray(Y) #output [2,4,7,10,15,...]\n","\n","# X_train Y_train placeholder  输入层\n","x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n","y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n","\n","# word embedding 词嵌入，维度为2\n","EMBEDDING_DIM = 2\n","\n","# hidden layer 隐藏层\n","#tf.random_normal 是一个用于生成服从正态分布（高斯分布）的随机数张量的函数。\n","W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n","b1 = tf.Variable(tf.random_normal([1])) # bias\n","#matmul 矩阵相乘\n","hidden_layer = tf.add(tf.matmul(x,W1), b1)\n","\n","\n","# output layer 输出层\n","W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n","b2 = tf.Variable(tf.random_normal([1]))\n","prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n","\n","# loss function : cross entropy 损失函数\n","loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n","\n","# training operation\n","train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n","##============= Tensorflow 开始训练\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","\n","iteration = 20000 #训练轮次\n","\n","for i in range(iteration) :\n","    # input : X_train(one-hot encoded word)\n","    # label : Y_train(one-hot encoded neighbor word) 对比\n","    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n","\n","    if i % 3000 == 0 :\n","        print('iteration ' + str(i) + ' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))\n","\n","#2D coordinates for the words 单词的二维坐标\n","vectors = sess.run(W1 + b1)\n","print(vectors)\n","#[[-3.0716753  -1.1877073 ]\n","#。。。。。\n","# [-0.80031365  0.11600411]]\n","\n","#word2vec\n","w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])  #把单词和二维坐标整合在一张表格\n","wordlist = list(words) #convert word set to list\n","w2v_df['word'] = wordlist\n","w2v_df = w2v_df[['word', 'x1', 'x2']]\n","print(w2v_df)\n","\n","##============= 画点图，找同义词\n","mport matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n","    ax.annotate(word, (x1,x2 ))\n","\n","PADDING = 1.0\n","x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n","y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n","x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n","y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n","\n","plt.xlim(x_axis_min,x_axis_max)\n","plt.ylim(y_axis_min,y_axis_max)\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","\n","plt.show()\n","##============="],"metadata":{"id":"j-nsLIXJfnqX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Session 6 Example code- Text Classification**"],"metadata":{"id":"rGHo0zSQfowM"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# 定义两个句子\n","sentences = [\n","    \"The cat sat on the mat.\",\n","    \"cats like mat.\",\n","    \"A dog chased the cat away.\"\n","]\n","\n","# 创建一个CountVectorizer对象来转换文本为BOW表示\n","vectorizer = CountVectorizer()\n","\n","# 对句子进行转换\n","bow_representation = vectorizer.fit_transform(sentences)\n","\n","# 输出词汇表\n","print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n","\n","# 输出BOW表示\n","print(\"BOW representation:\")\n","print(bow_representation.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PE4-zGXMUFS3","executionInfo":{"status":"ok","timestamp":1714418729581,"user_tz":-720,"elapsed":514,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"c1470bbf-2d2e-46a5-adec-e49fb2274c47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['away' 'cat' 'cats' 'chased' 'dog' 'like' 'mat' 'on' 'sat' 'the']\n","BOW representation:\n","[[0 1 0 0 0 0 1 1 1 2]\n"," [0 0 1 0 0 1 1 0 0 0]\n"," [1 1 0 1 1 0 0 0 0 1]]\n"]}]},{"cell_type":"code","source":["##============= 加载，清理\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","import re\n","import nltk\n","from sklearn.datasets import load_files\n","#nltk.download('stopwords')\n","import pickle\n","from nltk.corpus import stopwords\n","movie_data = load_files(r\"/content/drive/MyDrive/Colab Notebooks/teaching/data/txt_sentoken\") # folder containing the 2 categories of documents in individual folders.\n","X, y = movie_data.data, movie_data.target #加载语料库（根据标签，放入了不同文件夹，文件夹就是标签）\n","documents = []\n","\n","#清理+stemmer每篇文章，放入documents 数组\n","for sen in range(0, len(X)):\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(X[sen]))\n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    # Converting to Lowercase\n","    document = document.lower()\n","    # Lemmatization\n","    document = document.split()\n","    from nltk.stem import WordNetLemmatizer\n","    stemmer = WordNetLemmatizer()\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document)\n","    documents.append(document)\n","##============= vector data\n","#BOW\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n","X = vectorizer.fit_transform(documents).toarray()\n","#TFIDF\n","from sklearn.feature_extraction.text import TfidfTransformer\n","tfidfconverter = TfidfTransformer()\n","X = tfidfconverter.fit_transform(X).toarray()\n","\n","##=============#创建training 、 test 数据\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","#T##=============rain a classifier.  用随机森林训练一个分类器\n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n","classifier.fit(X_train, y_train)\n","\n","##=============#预测\n","y_pred = classifier.predict(X_test)\n","\n","##=============评价 evaluation metrics\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","print(confusion_matrix(y_test,y_pred))\n","print(classification_report(y_test,y_pred))\n","print(accuracy_score(y_test, y_pred))\n","print('FINISHED')\n","\n","##============="],"metadata":{"id":"CWzqcGYsfowM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Session 6 Example code  --CNN text classification**"],"metadata":{"id":"sRGitC0yNFkR"}},{"cell_type":"code","source":["##=============\n","'''\n","#This example demonstrates the use of Convolution1D for text classification.\n","2轮，准确率0.89； cpu 每轮90秒； GPU 每轮10秒\n","Gets to 0.89 test accuracy after 2 epochs. </br>\n","90s/epoch on Intel i5 2.4Ghz CPU. </br>\n","10s/epoch on Tesla K40 GPU.\n","'''\n","from __future__ import print_function\n","\n","from keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n","from keras.datasets import imdb\n","from keras.utils import pad_sequences\n","\n","# set parameters:\n","# max_features = 500\n","# maxlen = 400\n","# batch_size = 32\n","# embedding_dims = 5\n","# filters = 25\n","# kernel_size = 3\n","# hidden_dims = 250\n","# epochs = 2\n","\n","\n","# More real parameters:\n","max_features = 5000\n","maxlen = 400\n","batch_size = 32\n","embedding_dims = 50\n","filters = 250\n","kernel_size = 3\n","hidden_dims = 250\n","epochs = 2\n","\n","\n","\n","print('Loading data...') #加载数据\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences')\n","\n","print('Pad sequences (samples x time)') #取一样长数据\n","x_train = pad_sequences(x_train, maxlen=maxlen)\n","x_test = pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n","\n","print('Build model...') #开始建模\n","model = Sequential()\n","\n","# we start off with an efficient embedding layer which maps\n","# our vocab indices into embedding_dims dimensions #嵌入向量Embedding\n","model.add(Embedding(max_features,\n","                    embedding_dims,\n","                    input_length=maxlen))\n","#max_features：词汇表的大小。 即在训练数据中出现的不同单词的总数。\n","#embedding_dims：嵌入向量的维度。即每个单词将被表示为一个具有多少个特征的向量。\n","#input_length：输入序列的长度，即每个输入序列的单词数量，这个参数通常需要根据实际数据进行设置。\n","#因此，这段代码创建了一个嵌入层，用于将输入的整数序列（表示单词在词汇表中的索引）转换为密集的嵌入向量。\n","#     这些嵌入向量将作为神经网络的输入，并参与网络的训练过程，以学习单词之间的语义关系。\n","model.add(Dropout(0.2))\n","\n","# we add a Convolution1D, which will learn filters\n","# word group filters of size filter_length: #卷积层\n","model.add(Conv1D(filters,\n","                 kernel_size,\n","                 padding='valid',\n","                 activation='relu',\n","                 strides=1))\n","# we use max pooling: #最大池化\n","model.add(GlobalMaxPooling1D())\n","\n","# We add a vanilla hidden layer: 隐藏层\n","model.add(Dense(hidden_dims))\n","model.add(Dropout(0.2))\n","model.add(Activation('relu'))\n","\n","# We project onto a single unit output layer, and squash it with a sigmoid: #输出层\n","model.add(Dense(1))\n","#model.add(Activation('sigmoid'))\n","\n","#编译模型\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","#训模\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))\n","##=============模型总结\n","model.summary()\n","##============= plot 对比每轮准确率\n","hist = model.fit(x_test, y_test, validation_split=0.2, epochs=2, batch_size=20)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","sns.set()\n","\n","acc = hist.history['accuracy']\n","val = hist.history['val_accuracy']\n","epochs = range(1, len(acc) + 1)\n","plt.plot(epochs, acc, '-', label='Training accuracy')\n","plt.plot(epochs, val, ':', label='Validation accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')\n","plt.plot()\n","##============="],"metadata":{"id":"2CkRjlXFMszR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Session 7 Example code  --NER**\n","\n","\n"],"metadata":{"id":"4TkdYW65UEtN"}},{"cell_type":"code","source":["\n","##=====================================================NER using NLTK\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/TextMining/lab07/sampleText2.txt'\n","with open(DATA_DIR, 'r',encoding='utf-8', errors='ignore') as f:\n","     text1 = f.read()\n","sentences = nltk.sent_tokenize(text1)\n","print(\"sentences\",sentences)\n","tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","print(\"tokenized_sentences\",tokenized_sentences)\n","tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n","print(\"tagged_sentences:\",tagged_sentences)\n","chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n","print(\"chunked_sentences:\",chunked_sentences)\n","\n","def extract_entity_names(t):\n","    entity_names = []\n","\n","    if hasattr(t, 'label') and t.label:\n","        if t.label() == 'NE':\n","            entity_names.append(' '.join([child[0] for child in t]))\n","        else:\n","            for child in t:\n","                entity_names.extend(extract_entity_names(child))\n","\n","    return entity_names\n","\n","entity_names = []\n","for tree in chunked_sentences:\n","    #Print results per sentence\n","    #print(extract_entity_names(tree))\n","    entity_names.extend(extract_entity_names(tree))\n","\n","\n","\n","# Print all entity names\n","print(entity_names)\n","\n","# Print unique entity names\n","print (set(entity_names))\n","print(len(entity_names))\n","print(len(set(entity_names)))\n","\n","##===================================================== simple in NLTK\n","for sent in nltk.sent_tokenize(text1):\n","  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n","     if hasattr(chunk, 'label'):\n","        print(chunk.label(), ' '.join(c[0] for c in chunk))\n","'''\n","PERSON Alert\n","PERSON Grant Robertson\n","GPE Covid\n","GPE NZ\n","'''\n","##=====================================================Using Spacy Library\n","from os import forkpty\n","# # Using Spacy NER\n","import spacy  # Install spacy if required.\n","\n","nlp = spacy.load('en_core_web_sm')  # Install en_core_web_sm if required.\n","doc = nlp(text1)\n","\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","#Grant Robertson 101 116 PERSON\n","#12 146 148 CARDINAL\n","#today 188 193 DATE\n","#seven-day 199 208 DATE\n","\n","\n","file1 = open('/content/drive/MyDrive/Colab Notebooks/TextMining/lab07/deptokens.csv','w')\n","allTokens = ''\n","for token in doc:\n","    tokens = token.text + ',' + token.lemma_+ ',' + token.pos_ + ',' + token.tag_ + ',' + token.dep_ + ',' +\\\n","             token.shape_ + ',' +  str(token.is_alpha) + ',' + str(token.is_stop) + ',' + token.ent_type_ +'\\n'\n","    #print(tokens)\n","    allTokens = allTokens + tokens\n","file1.write(allTokens)\n","file1.close\n","print(allTokens)\n","\n","'''\n","Covid,Covid,PROPN,NNP,nmod,Xxxxx,True,False\n","19,19,NUM,CD,nummod,dd,False,False\n","Delta,Delta,PROPN,NNP,dobj,Xxxxx,True,False\n","outbreak,outbreak,NOUN,NN,ROOT,xxxx,True,False\n",":,:,PUNCT,:,punct,:,False,False\n","Alert,alert,ADJ,JJ,amod,Xxxxx,True,False\n","level,level,NOUN,NN,compound,xxxx,True,False\n","review,review,NOUN,NN,appos,xxxx,True,False\n","'''\n","\n","\n","#today,today,NOUN,NN,npadvmod,xxxx,True,False\n","\n","# filter doc.ents where ent.label_ == 'DATE'\n","\n","##====================================================="],"metadata":{"id":"QW2jSAuqUl-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Quiz Example code**\n","\n","\n"],"metadata":{"id":"Wm9g6uFZn_vG"}},{"cell_type":"code","source":["#======================================================quiz 03\n","import spacy\n","\n","\n","print(\"0===================================== chunks\")\n","\n","nlp = spacy.load('en_core_web_sm')\n","doc = nlp(\"COMP700 Quiz held in Auckland, New Zealand\")\n","for chunk in doc.noun_chunks:\n","    print(chunk.text, chunk.label_, '[', chunk.root.text, ']')\n","\n","print(\"1===================================== token\")\n","def printDocDep(doc):\n","  for token in doc:\n","    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n","\n","doc = nlp(\"I went to town on a bicycle\")\n","\n","printDocDep(doc)\n","print(\"2===================================== token\")\n","from spacy import displacy\n","from spacy.symbols import nsubj, VERB, dobj, pobj, NOUN,PROPN, PERSON\n","doc = nlp(\"The bus drove into the pole with speed\")\n","for token in doc:\n","  if token.dep == nsubj and token.head.pos == VERB:\n","    print([child for child in token.head.children]) # Tree navigation\n","\n","\n","print(\"3===================================== token, matcher\")\n","def printDocTags(doc):\n","  for token in doc:\n","    print(token.text,'/', token.pos_)\n","\n","from spacy.matcher import Matcher\n","doc = nlp('Johnny Jumped in the river')\n","#printDocDep(doc)\n","printDocTags(doc)\n","matcher = Matcher(vocab = nlp.vocab)\n","clause= [{'POS': 'PROPN'},{'POS': 'VERB'},{'POS': 'NOUN'}]\n","matcher.add('clause', patterns=[clause])\n","result = matcher(doc,as_spans=True)\n","print(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FIg43NZqmi5","executionInfo":{"status":"ok","timestamp":1714357960874,"user_tz":-720,"elapsed":847,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"34469960-f02e-4a06-baa8-c503c9ac13d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0===================================== chunks\n","Quiz NP [ Quiz ]\n","Auckland NP [ Auckland ]\n","New Zealand NP [ Zealand ]\n","1===================================== token\n","I/PRP <--nsubj-- went/VBD\n","went/VBD <--ROOT-- went/VBD\n","to/IN <--prep-- went/VBD\n","town/NN <--pobj-- to/IN\n","on/IN <--prep-- went/VBD\n","a/DT <--det-- bicycle/NN\n","bicycle/NN <--pobj-- on/IN\n","2===================================== token\n","[bus, into, with]\n","3===================================== token, matcher\n","Johnny / PROPN\n","Jumped / PROPN\n","in / ADP\n","the / DET\n","river / NOUN\n","[]\n"]}]},{"cell_type":"code","source":["#==========================================================quiz05,tensorflow embedding.\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()  # 禁用 TensorFlow 2.x 特性\n","print(\"Tensflow version:\",tf.__version__)  # 打印 TensorFlow 版本号\n","import numpy as np\n","# ?? words = \"\"\n","ONE_HOT_DIM = len(words)  # 获取单词列表的长度\n","\n","def to_one_hot_encoding(data_point_index):\n","    \"\"\"将索引转换为独热编码\"\"\"\n","    one_hot_encoding = np.zeros(ONE_HOT_DIM)  # 创建全零数组\n","    one_hot_encoding[data_point_index] = 1  # 设置指定索引位置为1\n","    return one_hot_encoding\n","\n","X = []\n","Y = []\n","\n","for x, y in zip(df['input'], df['label']):\n","    X.append(to_one_hot_encoding(word2int[ x ]))  # 将输入转换为独热编码并添加到X列表中\n","    Y.append(to_one_hot_encoding(word2int[ y ]))  # 将标签转换为独热编码并添加到Y列表中\n","\n","X_train = np.asarray(X)  # 转换为NumPy数组\n","Y_train = np.asarray(Y)  # 转换为NumPy数组\n","\n","x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))  # 创建输入占位符\n","y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))  # 创建标签占位符, length = len( word vocabulary)\n","\n","EMBEDDING_DIM = 300  # 设置嵌入维度 300\n","\n","W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))  # 创建第一个权重矩阵\n","b1 = tf.Variable(tf.random_normal([1]))  # 创建第一个偏置项\n","hidden_layer = tf.add(tf.matmul(x,W1), b1)  # 创建隐藏层\n","\n","W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))  # 创建第二个权重矩阵\n","b2 = tf.Variable(tf.random_normal([1]))  # 创建第二个偏置项\n","prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))  # 计算预测值,创建softmax\n","\n","loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))  # 计算交叉熵损失,它计算每个输入的误差并找到平均值。\n","\n","train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)  # 使用梯度下降优化器最小化损失函数\n","\n","sess = tf.Session()  # 创建 TensorFlow 会话\n","init = tf.global_variables_initializer()  # 初始化所有变量\n","sess.run(init)  # 运行初始化操作\n"],"metadata":{"id":"hfa-960wtYyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","#==========================================================quiz07 continueous chunking\n","from nltk import ne_chunk, pos_tag,word_tokenize\n","from nltk.tree import Tree\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","def get_continous_chunks(text):\n","  print(\"text= \", text)\n","  chunked = ne_chunk(pos_tag(word_tokenize(text)))\n","  print(\"word_tokenize(text)= \", word_tokenize(text))\n","  print(\"pos_tag(word_tokenize(text))= \", pos_tag(word_tokenize(text)))\n","  print(\"Parsing: ne_chunk(pos_tag(word_tokenize(text)))= \", chunked)\n","  print(\"1============================= \")\n","  prev = None\n","  continous_chuck =[]\n","  current_chunk = []\n","\n","  for i in chunked:\n","    if type(i) == Tree:\n","      print(i,\"===is tree, i.leaves()=\",i.leaves())\n","      current_chunk.append(\" \".join([token for token,pos in i.leaves()]))\n","    elif current_chunk:\n","      named_entity = \" \".join(current_chunk)\n","      if named_entity not in continous_chuck:\n","        continous_chuck.append(named_entity)\n","        current_chunk = []\n","    else:\n","      continue\n","\n","  if continous_chuck:\n","    named_entity = \" \".join(current_chunk)\n","    if named_entity not in continous_chuck:\n","        continous_chuck.append(named_entity)\n","\n","  return continous_chuck\n","\n","\n","txt = \"I couldn't do it. you can pre-processing it. Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't. Please correct  the result.\"\n","\n","print(get_continous_chunks(txt))\n","print(\"2=============================\")\n","\n","print(\"nltk.sent_tokenize(txt)= \", nltk.sent_tokenize(txt))\n","for sent in nltk.sent_tokenize(txt):\n","  print(\"sent= \", sent)\n","  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n","    print(\"chunk= \", chunk)\n","    if hasattr(chunk,'label'):\n","      print(chunk.label(),' '.join(c[0] for c in chunk))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8pG7HJKoGKi","executionInfo":{"status":"ok","timestamp":1714385096480,"user_tz":-720,"elapsed":407,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"8ae1003e-2567-4567-b615-093537c8bc0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["text=  I couldn't do it. you can pre-processing it. Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't. Please correct  the result.\n","word_tokenize(text)=  ['I', 'could', \"n't\", 'do', 'it', '.', 'you', 'can', 'pre-processing', 'it', '.', 'Jacinda', 'Ardern', 'is', 'the', 'Prime', 'Minister', 'of', 'New', 'Zealand', 'but', 'Roenzo', 'is', \"n't\", '.', 'Please', 'correct', 'the', 'result', '.']\n","pos_tag(word_tokenize(text))=  [('I', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('do', 'VB'), ('it', 'PRP'), ('.', '.'), ('you', 'PRP'), ('can', 'MD'), ('pre-processing', 'VB'), ('it', 'PRP'), ('.', '.'), ('Jacinda', 'NNP'), ('Ardern', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('Prime', 'NNP'), ('Minister', 'NNP'), ('of', 'IN'), ('New', 'NNP'), ('Zealand', 'NNP'), ('but', 'CC'), ('Roenzo', 'NNP'), ('is', 'VBZ'), (\"n't\", 'RB'), ('.', '.'), ('Please', 'VB'), ('correct', 'VB'), ('the', 'DT'), ('result', 'NN'), ('.', '.')]\n","Parsing: ne_chunk(pos_tag(word_tokenize(text)))=  (S\n","  I/PRP\n","  could/MD\n","  n't/RB\n","  do/VB\n","  it/PRP\n","  ./.\n","  you/PRP\n","  can/MD\n","  pre-processing/VB\n","  it/PRP\n","  ./.\n","  (PERSON Jacinda/NNP Ardern/NNP)\n","  is/VBZ\n","  the/DT\n","  Prime/NNP\n","  Minister/NNP\n","  of/IN\n","  (GPE New/NNP Zealand/NNP)\n","  but/CC\n","  (PERSON Roenzo/NNP)\n","  is/VBZ\n","  n't/RB\n","  ./.\n","  Please/VB\n","  correct/VB\n","  the/DT\n","  result/NN\n","  ./.)\n","1============================= \n","(PERSON Jacinda/NNP Ardern/NNP) ===is tree, i.leaves()= [('Jacinda', 'NNP'), ('Ardern', 'NNP')]\n","(GPE New/NNP Zealand/NNP) ===is tree, i.leaves()= [('New', 'NNP'), ('Zealand', 'NNP')]\n","(PERSON Roenzo/NNP) ===is tree, i.leaves()= [('Roenzo', 'NNP')]\n","['Jacinda Ardern', 'New Zealand', 'Roenzo', '']\n","2=============================\n","nltk.sent_tokenize(txt)=  [\"I couldn't do it.\", 'you can pre-processing it.', \"Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't.\", 'Please correct  the result.']\n","sent=  I couldn't do it.\n","chunk=  ('I', 'PRP')\n","chunk=  ('could', 'MD')\n","chunk=  (\"n't\", 'RB')\n","chunk=  ('do', 'VB')\n","chunk=  ('it', 'PRP')\n","chunk=  ('.', '.')\n","sent=  you can pre-processing it.\n","chunk=  ('you', 'PRP')\n","chunk=  ('can', 'MD')\n","chunk=  ('pre-processing', 'VB')\n","chunk=  ('it', 'PRP')\n","chunk=  ('.', '.')\n","sent=  Jacinda Ardern is the Prime Minister of New Zealand but Roenzo isn't.\n","chunk=  (PERSON Jacinda/NNP)\n","PERSON Jacinda\n","chunk=  (ORGANIZATION Ardern/NNP)\n","ORGANIZATION Ardern\n","chunk=  ('is', 'VBZ')\n","chunk=  ('the', 'DT')\n","chunk=  ('Prime', 'NNP')\n","chunk=  ('Minister', 'NNP')\n","chunk=  ('of', 'IN')\n","chunk=  (GPE New/NNP Zealand/NNP)\n","GPE New Zealand\n","chunk=  ('but', 'CC')\n","chunk=  (PERSON Roenzo/NNP)\n","PERSON Roenzo\n","chunk=  ('is', 'VBZ')\n","chunk=  (\"n't\", 'RB')\n","chunk=  ('.', '.')\n","sent=  Please correct  the result.\n","chunk=  ('Please', 'VB')\n","chunk=  ('correct', 'VB')\n","chunk=  ('the', 'DT')\n","chunk=  ('result', 'NN')\n","chunk=  ('.', '.')\n"]}]},{"cell_type":"code","source":["!python --version"],"metadata":{"id":"xuTtvg8PCm6F","executionInfo":{"status":"ok","timestamp":1712637222023,"user_tz":-720,"elapsed":6,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf943140-9e1d-421e-9851-055b88c87b52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('stopwords')\n","\n","nltk.download('punkt')\n","\n","def remove_stop_words(text):\n","\n","    from nltk.corpus import stopwords\n","\n","    from nltk.tokenize import word_tokenize\n","\n","    stop_words = stopwords.words('english')\n","\n","    newStopWords = ['it', 'its',',',':',';'] # add your own stop words to the list here.\n","\n","    stop_words = stop_words + newStopWords\n","\n","    word_tokens = word_tokenize(text)\n","\n","\n","\n","    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n","\n","    return filtered_sentence\n","\n","lines = 'When I did COMP700, I learnt about text and vision processing @ AUT.'\n","\n","cleanedLines = remove_stop_words(lines)\n","\n","print(cleanedLines)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8ZNTizX-dlF","executionInfo":{"status":"ok","timestamp":1714446538917,"user_tz":-720,"elapsed":5943,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"5f0a9458-3dbb-4aa2-ec3b-968a46f81cb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['COMP700', 'learnt', 'text', 'vision', 'processing', '@', 'AUT', '.']\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","text = [\"The quick brown fox jumped over the lazy dog.\",\n","\n","    \"The dog jumped over the fox.\",\n","\n","    \"The fox\"]\n","\n","vectorizer = TfidfVectorizer()\n","\n","vectorizer.fit(text)\n","\n","print(vectorizer.vocabulary_)\n","\n","print(\"vectorizer.idf_\")\n","print(vectorizer.idf_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ivbv2irBX_R","executionInfo":{"status":"ok","timestamp":1714447318323,"user_tz":-720,"elapsed":339,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"5b1fbeaa-3925-461a-a264-4e9328c033da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n","vectorizer.idf_\n","[1.69314718 1.28768207 1.         1.28768207 1.69314718 1.28768207\n"," 1.69314718 1.        ]\n"]}]},{"cell_type":"code","source":["text = nltk.word_tokenize(\"The quick brown fox jumped on the dog\")\n","def find_bigrams(input_list):\n","  bigram_list = []\n","  for i in range(len(input_list)-1):\n","      bigram_list.append((input_list[i], input_list[i+1]))\n","\n","#get individual items from the bigram\n","bigrams = find_bigrams(text)\n","print(bigrams)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LK19Kz8kF7I9","executionInfo":{"status":"ok","timestamp":1714448573390,"user_tz":-720,"elapsed":368,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"f3e9979b-c121-4d9a-fa63-c3ace4f2871e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","text = nltk.word_tokenize(\"The talk was boring\")\n","print(nltk.pos_tag(text))\n","\n","text = nltk.word_tokenize(\"You should talk more in class\")\n","print(nltk.pos_tag(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRWO2AKHHlRD","executionInfo":{"status":"ok","timestamp":1714449079715,"user_tz":-720,"elapsed":964,"user":{"displayName":"Kong Kong","userId":"12174995287770087580"}},"outputId":"3ea80a02-620f-454c-cccf-6ce2c47d108d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('The', 'DT'), ('talk', 'NN'), ('was', 'VBD'), ('boring', 'VBG')]\n","[('You', 'PRP'), ('should', 'MD'), ('talk', 'VB'), ('more', 'RBR'), ('in', 'IN'), ('class', 'NN')]\n"]}]}]}