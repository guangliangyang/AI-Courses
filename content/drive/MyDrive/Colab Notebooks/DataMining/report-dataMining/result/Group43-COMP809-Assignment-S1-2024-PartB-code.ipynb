{"cells":[{"cell_type":"markdown","source":["# COMP809 Assignment2-Part B\n","# Prediction of PM2.5 With  Multi-layer Perceptron (MLP) and Long Short-term  Memory (LSTM)\n","\n","\n","\n","*    Group 43\n","*    Yize(Serena) Wang, Student ID: 23198583\n","*    GuangLiang(Ricky) Yang, Student ID: 23205919\n","\n","\n","\n","\n","\n","\n","This notebook only contains code, please refer to the running results:\n","\n","1.  full dataset with line interpolation\n","https://drive.google.com/file/d/1CSe6pAywdErIWvxe9iS2EmOzyStftXNl/view?usp=sharing\n","\n","2. Full dataset with KNN interpolation\n","https://drive.google.com/file/d/188dLOxdTJKS_VN1HGgyPWTXC6rNsB8fM/view?usp=drive_link\n","\n","3. High qulity dataset with line interpolation\n","https://drive.google.com/file/d/1axAkZ6PKk0mNjPIxWoovJBLoC2zgArjx/view?usp=drive_link\n"],"metadata":{"id":"ZBxTdkyZqswf"}},{"cell_type":"markdown","source":["# **Project Workflow**\n","\n"],"metadata":{"id":"WpqUS9yPVvz9"}},{"cell_type":"markdown","source":["\n","1. **Pre-Processing**\n","    * Data Exploration\n","    * Data Cleaning\n","    * Extract New Features\n","    * Converting Categorical Data Types\n","    * Final Dataset\n","\n","2. **Feature Selection**\n","    * Correlation Analysis\n","    * Visualization\n","\n","3. **Experimental Methods**\n","    * Normalization Data\n","    * Data Segment\n","\n","4. **Multilayer Perceptron (MLP)**\n","    * MLP Description\n","    * Learning Rate Analysis\n","    * Neurons Distribution Analysis\n","    * MLP Conclusion\n","\n","5. **Long Short-Term Memory (LSTM)**\n","    * LSTM Introduction\n","    * Cost Function Analysis\n","    * Best Epoch Analysis\n","    * Batch Size Analysis\n","    * Number of Neurons in Hidden Layer Analysis\n","    * LSTM Conclusion\n","\n","6. **Model Comparison**\n","    * Visual Prediction Comparison\n","    * Performance Comparison\n"],"metadata":{"id":"9hCwjojoeSkL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IP9EyZ-0zwNI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Colab Notebooks/DataMining/report-dataMining/data/'"]},{"cell_type":"markdown","metadata":{"id":"Q5qYEUjrU5nM"},"source":["# **Step1: Pre-Processing**"]},{"cell_type":"markdown","source":["## **1.1 Data Exploration.**"],"metadata":{"id":"rQMwoAjwYyOl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nw2hLxFKU5nO"},"outputs":[],"source":["import time\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","from tabulate import tabulate\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import  StratifiedKFold,KFold\n","from sklearn.neural_network import MLPClassifier, MLPRegressor\n","from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report, mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.impute import KNNImputer\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# For confusion_matrix plot\n","import itertools\n","\n","# Keras libraries for LSTM\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dropout, Dense\n","from keras.optimizers import Adam\n","from keras.callbacks import Callback\n","from keras.losses import MeanSquaredError, MeanAbsoluteError, MeanSquaredLogarithmicError, Huber\n","\n","from tensorflow.keras.regularizers import l1, l2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L-E-bYfu10u"},"outputs":[],"source":["# Define a control parameter to decide whether to compute hyperparameters\n","CALCU_HYPER_PARAMETERS = True#False #True\n","# How many times the model is run to train\n","MODEL_RUN_COUNT = 30\n","# LSTM sliding window Length\n","N_STEPS = 2"]},{"cell_type":"markdown","metadata":{"id":"TNiD-sG2U5nO"},"source":["### Import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kp7Ri1mU5nP","scrolled":true},"outputs":[],"source":["data=pd.read_csv(path+'BulkExport-7-20240606201329-clean-excel-01.csv')\n","#BulkExport-7-20240606201329-clean-excel-01.csv   full data ， not clean\n","#BulkExport-7-20240606201329-clean-excel-02.csv   clean, 2022/3/18 - 2022/6/6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94tUo4DZU5nP"},"outputs":[],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPrVj68dU5nQ"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","source":["### exploration"],"metadata":{"id":"PqgdGAg4gfvp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhKdsQE72x_Q"},"outputs":[],"source":["def show_time_series_hist_plot(df,columns_to_plot = [\"PM2.5\", \"SO2\", \"NO\", \"NO2\", \"Temp\", \"Humidity\", \"Wind_Dir\", \"Wind_Speed\"]):\n","\n","    # Create the subplots\n","    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n","    fig.suptitle('Data Distribution', fontsize=16)\n","\n","    # Flatten the axes array for easier iteration\n","    axes = axes.flatten()\n","\n","    # Define units for each column\n","    units = {\n","        \"PM2.5\": \"μg/m3\",\n","        \"SO2\": \"μg/m3\",\n","        \"NO\": \"μg/m3\",\n","        \"NO2\": \"μg/m3\",\n","        \"Temp\": \"°C\",\n","        \"Humidity\": \"%\",\n","        \"Wind_Dir\": \"Degrees\",\n","        \"Wind_Speed\": \"m/s\"\n","    }\n","\n","    # Plot each column\n","    for i, column in enumerate(columns_to_plot):\n","        if i < len(axes):\n","            axes[i].hist(data[column].dropna(), bins=30, edgecolor='yellow', alpha=0.7)\n","            axes[i].set_title(f'Distribution of {column}')\n","            axes[i].set_xlabel(f'{column} ({units[column]})')\n","            axes[i].set_ylabel('Count')\n","\n","    # Adjust layout\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","    plt.show()\n","\n","show_time_series_hist_plot(data)"]},{"cell_type":"markdown","metadata":{"id":"KArAVQPjU5nS"},"source":["### Data Type convert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4YCdU0h0_f_"},"outputs":[],"source":["\n","# Assuming 'data' is already loaded into a DataFrame\n","df = pd.DataFrame(data)\n","\n","# Converting 'start_time' and 'end_time' to datetime\n","\n","\n","df['start_time'] = pd.to_datetime(df['start_time'], format='%Y/%m/%d %H:%M')\n","df['end_time'] = pd.to_datetime(df['end_time'], format='%Y/%m/%d %H:%M')\n","\n","# Converting other columns to float\n","columns_to_convert = ['Temp', 'Humidity', 'Wind_Speed', 'Wind_Dir', 'NO', 'NO2', 'SO2', 'PM2.5']\n","\n","# Ensuring all values can be converted to float\n","for column in columns_to_convert:\n","    df[column] = pd.to_numeric(df[column], errors='coerce')\n","\n","# Convert columns to float\n","df[columns_to_convert] = df[columns_to_convert].astype(float)\n","\n","# Check the data types and look for any conversion issues\n","print(df.info())\n","\n","# Check for any NaN values introduced during conversion\n","print(df[columns_to_convert].isna().sum())"]},{"cell_type":"markdown","source":["### Data Check"],"metadata":{"id":"CMBjoiQUacUJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoeKmH97U5nT"},"outputs":[],"source":["# data check\n","print(\"Number of observation: \", df.shape[0])# check dimension\n","print(\"Any NA value:\", df.isnull().values.any()); #check for missing values\n","print(\"Any row duplictaes:\",df.duplicated().any());#check for dupllicates rows\n","\n","\n","# Define the rules for identifying errors in each column\n","error_checks = {\n","    'Temp': \"Temperature should be between -20°C and 40°C\",\n","    'Humidity': \"Humidity should be between 0% and 100%\",\n","    'Wind_Speed': \"Wind speed should be between 0 and 60 m/s\",\n","    'Wind_Dir': \"Wind direction should be between 0 and 360 degrees\",\n","    'NO': \"NO should be non-negative\",\n","    'NO2': \"NO2 should be non-negative\",\n","    'SO2': \"SO2 should be non-negative\",\n","    'PM2.5': \"PM2.5 should be non-negative\"\n","}\n","\n","# Functions to check each rule\n","error_functions = {\n","    'Temp': lambda x: (x < -20) | (x > 40),\n","    'Humidity': lambda x: (x < 0) | (x > 100),\n","    'Wind_Speed': lambda x: (x < 0) | (x > 60),\n","    'Wind_Dir': lambda x: (x < 0) | (x >= 360),\n","    'NO': lambda x: x < 0,\n","    'NO2': lambda x: x < 0,\n","    'SO2': lambda x: x < 0,\n","    'PM2.5': lambda x: x < 0\n","}\n","\n","# Initialize lists to store results\n","columns = []\n","rules = []\n","error_counts = []\n","percentages = []\n","\n","# Calculate the number of error values and percentages for each rule\n","for column, rule in error_checks.items():\n","    error_count = error_functions[column](df[column]).sum()\n","    error_percentage = (error_count / len(df)) * 100\n","    columns.append(column)\n","    rules.append(rule)\n","    error_counts.append(error_count)\n","    percentages.append(error_percentage)\n","\n","# Combine the results into a DataFrame\n","error_data = pd.DataFrame({\n","    'Column Name': columns,\n","    'Rule': rules,\n","    'Error Count': error_counts,\n","    'Percentage': percentages\n","})\n","\n","# Print the error data\n","print(error_data)\n","\n"]},{"cell_type":"markdown","source":["### **Missing Values.**"],"metadata":{"id":"_bW9Da5jaiGf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUiIjt5mH2aF"},"outputs":[],"source":["# Calculate the number of missing values for each column\n","missing_values_count = df.isnull().sum()\n","\n","# Calculate the percentage of missing values for each column\n","missing_values_percentage = (df.isnull().sum() / len(df)) * 100\n","\n","# Combine the results into a DataFrame\n","missing_data = pd.DataFrame({\n","    'Missing Values': missing_values_count,\n","    'Percentage': missing_values_percentage\n","})\n","\n","# Sort the DataFrame by the number of missing values\n","missing_data = missing_data.sort_values(by='Missing Values', ascending=False)\n","\n","# Print the report\n","print(missing_data)\n"]},{"cell_type":"markdown","source":["### **Invalid Data**"],"metadata":{"id":"V4uRCiR0gyOO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BseJc5_zJ1bA"},"outputs":[],"source":["# Function to replace error values with NaN based on specified conditions\n","def replace_with_nan(df, column, condition):\n","    df[column] = df[column].apply(lambda x: x if condition(x) else None)\n","\n","# Define the error conditions for each column\n","error_conditions = {\n","    'Temp': lambda x: -20 <= x <= 40,\n","    'Humidity': lambda x: 0 <= x <= 100,\n","    'Wind_Speed': lambda x: 0 <= x <= 60,\n","    'Wind_Dir': lambda x: 0 <= x < 360,\n","    'NO': lambda x: x >= 0,\n","    'NO2': lambda x: x >= 0,\n","    'SO2': lambda x: x >= 0,\n","    'PM2.5': lambda x: x >= 0,\n","}\n","\n","# Replace error values with NaN for each column\n","for column, condition in error_conditions.items():\n","    replace_with_nan(df, column, condition)\n","\n","# Fill NaN values with the previous non-NaN value for each column\n","# df.fillna(method='ffill', inplace=True)\n","\n","# Display the number of error values for each column after replacement\n","print(\"Number of error values in 'Temp':\", ((df['Temp'] < -20) | (df['Temp'] > 40)).sum())\n","print(\"Number of error values in 'Humidity':\", ((df['Humidity'] < 0) | (df['Humidity'] > 100)).sum())\n","print(\"Number of error values in 'Wind_Speed':\", ((df['Wind_Speed'] < 0) | (df['Wind_Speed'] > 60)).sum())\n","print(\"Number of error values in 'Wind_Dir':\", ((df['Wind_Dir'] < 0) | (df['Wind_Dir'] >= 360)).sum())\n","print(\"Number of error values in 'NO':\", (df['NO'] < 0).sum())\n","print(\"Number of error values in 'NO2':\", (df['NO2'] < 0).sum())\n","print(\"Number of error values in 'SO2':\", (df['SO2'] < 0).sum())\n","print(\"Number of error values in 'PM2.5':\", (df['PM2.5'] < 0).sum())\n","\n","\n","df.isnull().sum()"]},{"cell_type":"markdown","source":["### **Outliers**"],"metadata":{"id":"j7BqemftaqNO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsAXWPbfKGiC"},"outputs":[],"source":["\n","def analyze_outliers(df):\n","    # Copy df to df_pm25\n","    df_pm25 = df.copy()\n","\n","    # Function to calculate and plot for each column\n","    def plot_parameters(df, parameters, lower_bound, upper_bound):\n","        num_parameters = len(parameters)\n","        cols = 2\n","        rows = (num_parameters + 1) // cols\n","        fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows), sharex=True)\n","\n","        axes = axes.flatten()  # Flatten the 2D array of axes to 1D\n","\n","        for i, parameter in enumerate(parameters):\n","            # Flag and remove extreme values\n","            df['is_extreme'] = ((df[parameter] > upper_bound[parameter]) | (df[parameter] < lower_bound[parameter]))\n","            df_cleaned = df[~df['is_extreme']]\n","\n","            # Plot parameter trend over time with upper and lower limits\n","            axes[i].plot(df[\"start_time\"], df[parameter], label=parameter, color='blue')\n","            axes[i].axhline(y=upper_bound[parameter], color='red', linewidth=1, label='Upper Limit')\n","            axes[i].axhline(y=lower_bound[parameter], color='green', linewidth=1, label='Lower Limit')\n","            axes[i].set_ylabel(f\"{parameter} (unit)\")  # Change the unit according to the parameter\n","            axes[i].legend()\n","\n","        # Hide unused subplots\n","        for j in range(i + 1, len(axes)):\n","            fig.delaxes(axes[j])\n","\n","        plt.xlabel(\"Time\")\n","        plt.suptitle(\"Line plot of measured parameters with extreme values thresholds\")\n","        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","        plt.show()\n","\n","     # Remove the time columns for analysis if they exist\n","    time_columns = ['start_time', 'end_time']\n","    df_without_time = df.drop(columns=[col for col in time_columns if col in df.columns])\n","\n","\n","    # Describe the DataFrame to get the statistical summary\n","    stats = df_without_time.describe().T\n","\n","    # Calculate the IQR (Interquartile Range) for each column\n","    IQR = stats['75%'] - stats['25%']\n","\n","    # Define the lower and upper bounds for outliers\n","    lower_bound = stats['25%'] - 1.5 * IQR\n","    upper_bound = stats['75%'] + 1.5 * IQR\n","\n","    # Initialize lists to store results\n","    columns = []\n","    outlier_counts = []\n","    percentages = []\n","\n","    # Calculate the number of outliers and percentages for each column\n","    for column in df_without_time.columns:\n","        outlier_count = ((df_without_time[column] < lower_bound[column]) | (df_without_time[column] > upper_bound[column])).sum()\n","        outlier_percentage = (outlier_count / len(df)) * 100\n","        columns.append(column)\n","        outlier_counts.append(outlier_count)\n","        percentages.append(outlier_percentage)\n","\n","    # Combine the results into a DataFrame\n","    outlier_data = pd.DataFrame({\n","        'Column Name': columns,\n","        'Outlier Count': outlier_counts,\n","        'Percentage': percentages\n","    })\n","\n","    # Print the outlier data\n","    print(outlier_data)\n","\n","    # Generate the boxplot for all parameters\n","    fig, ax = plt.subplots(figsize=(15, 6))\n","    df_without_time.boxplot(ax=ax)\n","\n","    # Set the title and labels of the plot\n","    plt.title(\"Boxplot of Air Quality and Meteorological Data\", fontsize=16, fontweight='bold')\n","    plt.xlabel(\"Measured Parameters\", fontsize=14, fontweight='bold')\n","    plt.ylabel(\"Observed Values\", fontsize=14, fontweight='bold')\n","    plt.xticks(rotation=45, fontsize=12)\n","    plt.yticks(fontsize=12)\n","\n","    # Display the plot\n","    plt.show()\n","\n","    # Generate combined plot for all parameters\n","    plot_parameters(df_pm25, df_without_time.columns, lower_bound, upper_bound)\n","\n","\n","# show\n","analyze_outliers(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-sNJZXNU5nT"},"outputs":[],"source":["\n","df_without_time = df.drop(columns=['start_time','end_time'])\n","df_without_time.describe().T\n","\n"]},{"cell_type":"markdown","source":["## **1.2 Data Cleaning.**\n"],"metadata":{"id":"zec39ew-a4CW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKV_ceUaU5nU"},"outputs":[],"source":["# Define invalid data conditions\n","invalid_conditions = {\n","    'Temp': lambda x: (x < -20) | (x > 40),\n","    'Humidity': lambda x: (x < 0) | (x > 100),\n","    'Wind_Speed': lambda x: (x < 0) | (x > 60),\n","    'Wind_Dir': lambda x: (x < 0) | (x >= 360),\n","    'NO': lambda x: x < 0,\n","    'NO2': lambda x: x < 0,\n","    'SO2': lambda x: x < 0,\n","    'PM2.5': lambda x: x < 0\n","}\n","\n","# Replace invalid data with NaN\n","for col, condition in invalid_conditions.items():\n","    df.loc[condition(df[col]), col] = np.nan\n","\n","# Remove the time columns for analysis\n","df_without_time = df.drop(columns=['start_time', 'end_time'])\n","\n","# Describe the DataFrame to get the statistical summary\n","stats = df_without_time.describe().T\n","\n","# Calculate the IQR (Interquartile Range) for each column\n","IQR = stats['75%'] - stats['25%']\n","\n","# Define the lower and upper bounds for outliers\n","lower_bound = stats['25%'] - 1.5 * IQR\n","upper_bound = stats['75%'] + 1.5 * IQR\n","\n","# Replace outliers with NaN\n","for column in df_without_time.columns:\n","    df.loc[(df_without_time[column] < lower_bound[column]) | (df_without_time[column] > upper_bound[column]), column] = np.nan\n","\n","# Verify the updated dataset\n","print(df.info())\n"]},{"cell_type":"markdown","source":["### Result after replacing invalid data with NaN"],"metadata":{"id":"gXNz7dwQhDQW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGqhXS9-U5nV"},"outputs":[],"source":["\n","# Create a figure and a set of subplots with a specified size\n","fig, ax = plt.subplots(figsize=(15, 6))\n","\n","# Generate a boxplot of the DataFrame df on the specified axes ax\n","df.boxplot(ax=ax)\n","\n","# Set the title of the plot with a more descriptive title\n","plt.title(\"Boxplot of Air Quality and Meteorological Data\", fontsize=16, fontweight='bold')\n","\n","# Set the x-axis label with a more descriptive label\n","plt.xlabel(\"Measured Parameters\", fontsize=14, fontweight='bold')\n","\n","# Set the y-axis label with a more descriptive label\n","plt.ylabel(\"Observed Values\", fontsize=14, fontweight='bold')\n","\n","# Optionally, rotate the x-axis labels for better readability\n","plt.xticks(rotation=45, fontsize=12)\n","\n","# Set the y-axis label font size\n","plt.yticks(fontsize=12)\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","source":["### Generate a complete hourly frequency data source"],"metadata":{"id":"l6BBNqIAu9WT"}},{"cell_type":"code","source":["\n","# Title: Checking for Continuous Time Series and Inserting Missing Time Points\n","# Description: This script checks if the 'start_time' values in a dataframe are continuous.\n","# If not, it inserts the missing time points with hourly frequency."],"metadata":{"id":"5ru0t2g9uobh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqdswCzMU5nV"},"outputs":[],"source":["\n","df_without_time = df.drop(columns=['start_time','end_time'])\n","df_without_time.describe().T\n"]},{"cell_type":"code","source":["\n"],"metadata":{"id":"q3oL7j-MJk1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Imputation Method Comparison"],"metadata":{"id":"DcvD1rUnhQAz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYQKy3wIalK_"},"outputs":[],"source":["\n","\n","# Function to show time series plot\n","def show_time_series_plot(df, title):\n","    columns_to_plot = ['NO', 'NO2', 'SO2', 'Temp', 'Humidity', 'Wind_Speed', 'Wind_Dir', 'PM2.5']\n","    fig, axes = plt.subplots(nrows=len(columns_to_plot), ncols=1, figsize=(12, 10), sharex=True)\n","\n","    for i, col in enumerate(columns_to_plot):\n","        axes[i].plot(df['start_time'], df[col], label=col)\n","        axes[i].set_ylabel(col, fontsize=12)\n","        axes[i].legend(loc='upper right')\n","\n","    axes[-1].set_xlabel('Time', fontsize=12)\n","    fig.suptitle(title, fontsize=16, fontweight='bold')\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","    plt.show()\n","\n","# Handle Wind_Dir using circular statistics\n","def impute_wind_direction(df):\n","    # Convert wind direction to sine and cosine components\n","    df['Wind_Dir_sin'] = np.sin(np.deg2rad(df['Wind_Dir']))\n","    df['Wind_Dir_cos'] = np.cos(np.deg2rad(df['Wind_Dir']))\n","\n","    # Interpolate the sine and cosine components\n","    df['Wind_Dir_sin'] = df['Wind_Dir_sin'].interpolate(method='linear', limit_direction='both')\n","    df['Wind_Dir_cos'] = df['Wind_Dir_cos'].interpolate(method='linear', limit_direction='both')\n","\n","    # Convert back to wind direction\n","    df['Wind_Dir'] = np.rad2deg(np.arctan2(df['Wind_Dir_sin'], df['Wind_Dir_cos']))\n","    df['Wind_Dir'] = df['Wind_Dir'] % 360  # Ensure the values are within [0, 360]\n","    df.drop(columns=['Wind_Dir_sin', 'Wind_Dir_cos'], inplace=True)\n","    return df\n","\n","# Show initial time series plot\n","show_time_series_plot(df, 'Time Series Data of Air Quality and Meteorological (Original)')\n","\n","# Linear interpolation to repair the missing values\n","columns_to_impute = ['Temp', 'Humidity', 'Wind_Speed', 'NO', 'NO2', 'SO2', 'PM2.5']\n","\n","df_interpolated = df.copy()\n","df_interpolated[columns_to_impute] = df_interpolated[columns_to_impute].interpolate(method='linear', limit_direction='both')\n","df_interpolated = impute_wind_direction(df_interpolated)\n","\n","# Show time series plot after linear interpolation\n","show_time_series_plot(df_interpolated, 'Time Series Data of Air Quality and Meteorological (Linear Interpolation)')\n","\n","# KNN to repair the missing values\n","imputer = KNNImputer(n_neighbors=5)\n","df_knn = df.copy()\n","df_knn[columns_to_impute] = imputer.fit_transform(df_knn[columns_to_impute])\n","df_knn = impute_wind_direction(df_knn)\n","\n","# Show time series plot after KNN interpolation\n","show_time_series_plot(df_knn, 'Time Series Data of Air Quality and Meteorological (KNN Interpolation)')\n","\n","# Calculate errors\n","def calculate_errors(original, imputed):\n","    mse = mean_squared_error(original, imputed)\n","    mae = mean_absolute_error(original, imputed)\n","    return mse, mae\n","\n","df_original = df.fillna(method='bfill').fillna(method='ffill')  # Use backfill and forward fill for demonstration\n","errors = []\n","\n","for col in columns_to_impute + ['Wind_Dir']:\n","    mse_linear, mae_linear = calculate_errors(df_original[col], df_interpolated[col])\n","    mse_knn, mae_knn = calculate_errors(df_original[col], df_knn[col])\n","    errors.append([col, mse_linear, mae_linear, mse_knn, mae_knn])\n","\n","# Create a DataFrame for error comparison\n","errors_df = pd.DataFrame(errors, columns=['Column', 'MSE_Linear', 'MAE_Linear', 'MSE_KNN', 'MAE_KNN'])\n","\n","# Print the errors DataFrame\n","print(errors_df)\n","\n","# Visualize the comparison\n","errors_df.set_index('Column').plot(kind='bar', figsize=(12, 6))\n","plt.title('Error Comparison of Imputation Methods')\n","plt.ylabel('Error')\n","plt.xlabel('Columns')\n","plt.xticks(rotation=45)\n","plt.legend(loc='upper right')\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","source":["### After Data Imputation"],"metadata":{"id":"4mdKBObKhakz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gobn0T7XfHBP"},"outputs":[],"source":["# we choose line interpolated result\n","df = df_interpolated\n","df = df_knn #test knn\n","\n","analyze_outliers(df)"]},{"cell_type":"markdown","source":["## **1.3 Extract New Features.**\n"],"metadata":{"id":"MMoXQhjmbKRk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Lu0VPngnmSH"},"outputs":[],"source":["# Creating lag1 (1 hours before) and lag2 (2 hours before) for PM2.5\n","df['lag1'] = df['PM2.5'].shift(1)\n","df['lag2'] = df['PM2.5'].shift(2)\n","df['lag1_time'] = df['start_time'].shift(1)\n","df['lag2_time'] = df['start_time'].shift(2)\n","\n","# Setting the target to be the PM2.5 value next 1 hours into the future\n","#df['PM2.5'] = df['PM2.5'].shift(-1)\n","#df['target_time'] = df['start_time'].shift(-1)\n","\n","# Ensuring the time shifts are accurate\n","df['lag1'] = np.where((df['start_time'] - pd.Timedelta(hours=1)) != df['lag1_time'], np.nan, df['lag1'])\n","df['lag2'] = np.where((df['start_time'] - pd.Timedelta(hours=2)) != df['lag2_time'], np.nan, df['lag2'])\n","#df['PM2.5'] = np.where((df['start_time'] + pd.Timedelta(hours=1)) != df['target_time'], np.nan, df['PM2.5'])\n","\n","# Dropping the temporary time columns used for validation\n","df = df.drop(columns=['lag1_time', 'lag2_time']) #, 'target_time'\n","\n","# Create Month and Day_Of_Week columns\n","df['Month'] = df['start_time'].dt.month\n","df['Day_Of_Week'] = df['start_time'].dt.dayofweek\n","\n","# Drop rows where PM2.5, lag1, or lag2 are NaN\n","df = df.dropna(subset=['lag1', 'lag2']) #'PM2.5',\n","\n","# Verify the updated DataFrame\n","print(df.info())\n","print(df.head())\n"]},{"cell_type":"markdown","source":["## **1.4 Converting Categorical Data Types.**\n"],"metadata":{"id":"U9HYy55sbXJB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1YyFtrbC-n7"},"outputs":[],"source":["# Define a function to categorize wind direction\n","def categorize_wind_direction(num):\n","  val=int((num/22.5)+.5)\n","  arr=[\"N\",\"NNE\",\"NE\",\"ENE\",\"E\",\"ESE\", \"SE\", \"SSE\",\"S\",\"SSW\",\"SW\",\"WSW\",\"W\",\"WNW\",\"NW\",\"NNW\"]\n","  return arr[(val % 16)]\n","\n","df2 = df.copy()\n","\n","\n","# Apply the function to create a categorical wind direction column in the copy\n","df2['Wind_Type'] = df2['Wind_Dir'].apply(categorize_wind_direction)\n","\n","# Apply one-hot encoding for Wind_Type, Month, and Day_Of_Week with custom prefixes\n","df2 = pd.get_dummies(df2, columns=['Wind_Type', 'Month', 'Day_Of_Week'],\n","                     prefix=['Wind_Type', 'Month', 'Day'])\n","\n","# Convert only the dummy columns to float64\n","dummy_columns = [col for col in df2.columns if 'Wind_Type_' in col or 'Month_' in col or 'Day_' in col]\n","df2[dummy_columns] = df2[dummy_columns].astype('float64')\n","\n","# Drop the original Wind_Dir, Month, and Day_Of_Week columns in the copy as they are now encoded\n","df2 = df2.drop(columns=['Wind_Dir'])\n","\n","# Rename the columns to remove '.0'\n","df2.columns = df2.columns.str.replace('.0', '', regex=False)\n","# Print the information of the modified copy to confirm\n","df2.info()\n","df2.head()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NT-Mf0oTKrx1"},"outputs":[],"source":["#made our choice , use the linear interpolate\n","df2.isnull().sum()\n","\n","df2.info()\n","df2.head()"]},{"cell_type":"markdown","source":["## **1.5 Final Dataset.**\n"],"metadata":{"id":"oBWusesEbl1H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeJpdsdfed6X"},"outputs":[],"source":["#change the index\n","\n","df = df2.copy()\n","# Set start_time as index\n","df.set_index('start_time', inplace=True)\n","\n","# Remove end_time column\n","df.drop(columns=['end_time'], inplace=True)\n","\n","# Display the updated DataFrame\n","print(df.head())\n","df.info()\n","\n","show_time_series_hist_plot(df,columns_to_plot = [\"PM2.5\", \"SO2\", \"NO\", \"NO2\", \"Temp\", \"Humidity\",  \"Wind_Speed\"])\n"]},{"cell_type":"code","source":["\n","#df_without_time = df.drop(columns=['start_time','end_time'])\n","df_without_time.describe().T"],"metadata":{"id":"4CAC9uoXTQIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sMoLDWTXU5nV"},"source":["# **Step 2. Feature Selection**\n","\n"]},{"cell_type":"markdown","source":["\n","## **2.1 Correlation Analysis.**"],"metadata":{"id":"CCFR4nkGesrX"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"S-uC1YYLxNrv"},"outputs":[],"source":["\n","# Assuming df is the DataFrame after final preprocessing\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Create a copy of the DataFrame to scale the data\n","df_scaled = df.copy()\n","\n","# Select columns to scale (excluding the 'start_time' and 'end_time' if present)\n","columns_to_scale = df_scaled.columns.difference(['start_time', 'end_time'])\n","\n","# Apply the scaler to the selected columns in the copied DataFrame\n","df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n","\n","# Calculate Spearman correlations\n","spearman_corr = df_scaled.corr(method='spearman')\n","\n","# Extract Spearman correlations with PM2.5\n","spearman_corr_pm25 = spearman_corr.loc['PM2.5'].drop('PM2.5')\n","\n","# Create a DataFrame to store the Spearman correlation results\n","spearman_corr_df = pd.DataFrame({\n","    'Parameters': spearman_corr_pm25.index,\n","    'Spearman_Correlation': spearman_corr_pm25.values,\n","})\n","\n","# Order by absolute value of Spearman correlation while keeping the original values\n","spearman_corr_df['Absolute Spearman Correlation'] = spearman_corr_df['Spearman_Correlation'].abs()\n","spearman_corr_df = spearman_corr_df.sort_values(by='Absolute Spearman Correlation', ascending=False).drop(columns='Absolute Spearman Correlation')\n","\n","# Print the Spearman correlation table\n","spearman_table = tabulate(spearman_corr_df, headers='keys', tablefmt='plain', showindex=False)\n","print(\"Spearman Correlation Table:\")\n","print(spearman_table)\n","\n","# Visualize the ordered Spearman correlations with a bar plot\n","plt.figure(figsize=(10, 8))\n","sns.barplot(x='Spearman_Correlation', y='Parameters', data=spearman_corr_df, palette='coolwarm')\n","plt.title('Spearman Correlation with PM2.5')\n","plt.show()\n","\n","# Calculate Pearson correlations\n","pearson_corr = df_scaled.corr(method='pearson')\n","\n","# Extract Pearson correlations with PM2.5\n","pearson_corr_pm25 = pearson_corr.loc['PM2.5'].drop('PM2.5')\n","\n","# Create a DataFrame to store the Pearson correlation results\n","pearson_corr_df = pd.DataFrame({\n","    'Parameters': pearson_corr_pm25.index,\n","    'Pearson_Correlation': pearson_corr_pm25.values,\n","})\n","\n","# Order by absolute value of Pearson correlation while keeping the original values\n","pearson_corr_df['Absolute Pearson Correlation'] = pearson_corr_df['Pearson_Correlation'].abs()\n","pearson_corr_df = pearson_corr_df.sort_values(by='Absolute Pearson Correlation', ascending=False).drop(columns='Absolute Pearson Correlation')\n","\n","# Print the Pearson correlation table\n","pearson_table = tabulate(pearson_corr_df, headers='keys', tablefmt='plain', showindex=False)\n","print(\"Pearson Correlation Table:\")\n","print(pearson_table)\n","\n","# Visualize the ordered Pearson correlations with a bar plot\n","plt.figure(figsize=(10, 8))\n","sns.barplot(x='Pearson_Correlation', y='Parameters', data=pearson_corr_df, palette='coolwarm')\n","plt.title('Pearson Correlation with PM2.5')\n","plt.show()\n","\n","# Select top 8 parameters based on absolute Spearman correlation with PM2.5\n","top_8_columns = spearman_corr_df.head(8)['Parameters'].tolist()\n","\n","# Visualize the Spearman correlations with a heatmap for top 8 correlated features\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(spearman_corr.loc[top_8_columns + ['PM2.5'], top_8_columns + ['PM2.5']], annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10}, cbar_kws={'shrink': .5})\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.title('Spearman Correlation Matrix (Top 8 Features)', fontsize=20)\n","plt.show()\n","\n","# Visualize the Pearson correlations with a heatmap for top 8 correlated features\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(pearson_corr.loc[top_8_columns + ['PM2.5'], top_8_columns + ['PM2.5']], annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10}, cbar_kws={'shrink': .5})\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.title('Pearson Correlation Matrix (Top 8 Features)', fontsize=20)\n","plt.show()\n","\n","# Provide summary statistics of PM2.5 and the chosen predictors\n","summary_stats = df_scaled[['PM2.5'] + top_8_columns].describe()\n","print(summary_stats)\n","\n","# Scatter plots with random sampling of 100 data points\n","df_sampled = df_scaled.sample(n=100, random_state=42)\n","fig, axs = plt.subplots(4, 2, figsize=(20, 20))\n","\n","for i, feature in enumerate(top_8_columns):\n","    row, col_index = divmod(i, 2)\n","    sns.scatterplot(x=df_sampled[feature], y=df_sampled['PM2.5'], ax=axs[row, col_index])\n","    axs[row, col_index].set_title(f'Scatter plot between PM2.5 and {feature}')\n","    axs[row, col_index].set_xlabel(feature)\n","    axs[row, col_index].set_ylabel('PM2.5 (µg/m³)')\n","\n","# Remove empty subplots\n","for j in range(i + 1, len(axs.flatten())):\n","    fig.delaxes(axs.flatten()[j])\n","\n","# Adjust the layout\n","plt.tight_layout()\n","plt.show()\n","\n","# Multiple Linear Regression Analysis using scaled data\n","X = df_scaled[top_8_columns]\n","y = df_scaled['PM2.5']\n","X = sm.add_constant(X)  # Adds a constant term to the predictors\n","\n","model = sm.OLS(y, X).fit()\n","predictions = model.predict(X)\n","\n","print(f'Regression Analysis for PM2.5 with Multiple Features')\n","print(model.summary())\n","\n","# Residual plot\n","plt.figure(figsize=(10, 6))\n","sns.residplot(x=predictions, y=model.resid, lowess=True, color='g')\n","plt.title('Residual Plot for PM2.5 with Multiple Features')\n","plt.xlabel('Fitted values')\n","plt.ylabel('Residuals')\n","plt.show()\n","\n","# QQ plot\n","sm.qqplot(model.resid, line='45')\n","plt.title('QQ Plot for PM2.5 with Multiple Features')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQWERNKZq7G-"},"outputs":[],"source":["df.describe().T\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIO2I50d6Jpi"},"outputs":[],"source":["\n","#selected_features = corr_df['Parameters'][:5].tolist()\n","selected_features = ['lag1', 'lag2', 'NO', 'NO2', 'SO2']\n","print(\"Selected Features:\", selected_features)"]},{"cell_type":"code","source":["# Assuming df is the preprocessed DataFrame and start_time is the index\n","\n","# Step 1: Line plot showing the variation of PM2.5 concentration over time\n","plt.figure(figsize=(15, 7))\n","plt.plot(df.index, df['PM2.5'], color='blue')\n","plt.xlabel('Time')\n","plt.ylabel('PM2.5 Concentration (µg/m³)')\n","plt.title('PM2.5 Concentration Over Time')\n","plt.show()\n","\n","# Step 2: Box plot showing the distribution of PM2.5 concentration for each year\n","# Extract year information\n","df['Year'] = df.index.year\n","\n","# Box plot showing the distribution of PM2.5 concentration for each year\n","plt.figure(figsize=(15, 7))\n","sns.boxplot(x='Year', y='PM2.5', data=df)\n","plt.xlabel('Year')\n","plt.ylabel('PM2.5 Concentration (µg/m³)')\n","plt.title('Yearly PM2.5 Concentration Distribution')\n","plt.show()\n","\n","# Step 3: Summary statistics of PM2.5 concentration\n","# Provide descriptive statistics\n","pm25_summary_stats = df['PM2.5'].describe()\n","print(\"PM2.5 Concentration Summary Statistics:\")\n","print(pm25_summary_stats)\n","\n","# Step 4: Summary statistics of predictors\n","# Provide descriptive statistics for the predictors with the highest correlation\n","predictor_summary_stats = df[selected_features].describe()\n","print(\"Top Predictors Summary Statistics:\")\n","print(predictor_summary_stats)\n","\n","# Convert summary statistics results to tabular form and print\n","summary_table = predictor_summary_stats.transpose()\n","summary_table['Mean'] = summary_table['mean']\n","summary_table['Median'] = summary_table['50%']\n","summary_table['Std'] = summary_table['std']\n","summary_table['Min'] = summary_table['min']\n","summary_table['Max'] = summary_table['max']\n","summary_table['25%'] = summary_table['25%']\n","summary_table['75%'] = summary_table['75%']\n","\n","# Select only the needed columns\n","summary_table = summary_table[['Mean', 'Median', 'Std', 'Min', 'Max', '25%', '75%']]\n","\n","# Print the table\n","print(tabulate(summary_table, headers='keys', tablefmt='plain'))\n"],"metadata":{"id":"BJZrow2BHLcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kP3VsuLxU5nb"},"source":["# **Step 3. Experimental Methods**\n"]},{"cell_type":"markdown","metadata":{"id":"8dxz_imxU5nW"},"source":["\n","    * Normalization Data.\n","\n","    * Data segment\n","\n","Use 70% of the data for training and the rest for testing the MLP and LSTM models. Use a Workflow diagram to illustrate the process of predicting PM concentrations using the MLP and LSTM models."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"QaB01VFwU5nX"},"outputs":[],"source":["\n","# Initialize MinMaxScaler\n","my_scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","# Fit and transform the data\n","scaled_df = my_scaler.fit_transform(df[selected_features + ['PM2.5']])\n","scaled_df = pd.DataFrame(scaled_df, columns=selected_features + ['PM2.5'])\n","\n","# Extract features and target values\n","X_scaler = scaled_df[selected_features].values\n","y_scaler = scaled_df['PM2.5'].values\n","\n","\n","# Split the data into training and testing sets based on time order\n","train_size = int(len(X_scaler) * 0.7)\n","X_train, X_test = X_scaler[:train_size], X_scaler[train_size:]\n","y_train, y_test = y_scaler[:train_size], y_scaler[train_size:]\n","\n","\n","# Get the corresponding time indices for the train and test sets\n","time_indices = df.index\n","# Adjust time_test for sliding window\n","time_test = time_indices[train_size + N_STEPS:]\n","\n","\n","#===\n","\n","\n","# Display the normalized data\n","print(scaled_df.describe())\n","\n","#print data shape\n","print(\"X_train = \", X_train.shape)\n","print(\"X_test = \", X_test.shape)\n","print(\"y_train = \", y_train.shape)\n","print(\"y_test = \", y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"oDZ66M2HU5nb"},"source":["# **Step 4. Multilayer Perceptron (MLP)**"]},{"cell_type":"markdown","metadata":{"id":"THJGNHSFC961"},"source":["## 4.2 Learning Rate Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5XHHMEzC3zK"},"outputs":[],"source":["\n","# Define a function to evaluate the model\n","def evaluate_model(learning_rate):\n","    mlp = MLPRegressor(hidden_layer_sizes=(25,), learning_rate_init=learning_rate, max_iter=1000, random_state=42)\n","    mlp.fit(X_train, y_train)\n","\n","    y_pred_test = mlp.predict(X_test)\n","\n","    mse_test = mean_squared_error(y_test, y_pred_test)\n","    rmse_test = np.sqrt(mse_test)\n","    mae_test = mean_absolute_error(y_test, y_pred_test)\n","    r2_test = r2_score(y_test, y_pred_test)\n","\n","    return mse_test, rmse_test, mae_test, r2_test\n","\n","# List of learning rates to test\n","learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]\n","results = []\n","\n","for lr in learning_rates:\n","    mse, rmse, mae, r2 = evaluate_model(lr)\n","    results.append((lr, mse, rmse, mae, r2))\n","\n","# Convert results to a DataFrame for better visualization\n","results_df = pd.DataFrame(results, columns=['Learning Rate', 'MSE', 'RMSE', 'MAE', 'R²'])\n","\n","# Highlight the best learning rate\n","best_lr = results_df.loc[results_df['MSE'].idxmin()]\n","print(\"The best learning rate is: \", best_lr['Learning Rate'])\n","print(f\"Results for the best learning rate ({best_lr['Learning Rate']}):\")\n","print(f\"MSE: {best_lr['MSE']}\")\n","print(f\"RMSE: {best_lr['RMSE']}\")\n","print(f\"MAE: {best_lr['MAE']}\")\n","print(f\"R²: {best_lr['R²']}\")\n","\n","# Display the results\n","print(results_df)\n","\n","My_Learning_Rate = best_lr['Learning Rate']"]},{"cell_type":"markdown","metadata":{"id":"NWVq_euHEDoW"},"source":["\n","The best learning rate for the MLPRegressor model, based on the tested values, is 0.001, which resulted in the lowest MSE of 3.171270. Additionally, this learning rate provided an RMSE of 1.780806, an MAE of 1.318530, and an R² score of 0.311761. This rate should be used for the highest performance on the testing dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uu457pfZIOgI"},"outputs":[],"source":["\n","#if CALCU_HYPER_PARAMETERS:\n","    # Plotting the results\n","plt.figure(figsize=(12, 6))\n","plt.plot(results_df['Learning Rate'], results_df['MSE'], label='MSE')\n","plt.plot(results_df['Learning Rate'], results_df['RMSE'], label='RMSE')\n","plt.plot(results_df['Learning Rate'], results_df['MAE'], label='MAE')\n","plt.xlabel('Learning Rate')\n","plt.ylabel('Error')\n","plt.title('Error Metrics vs Learning Rate')\n","plt.legend()\n","plt.xscale('log')  # To better visualize the differences across learning rates\n","plt.show()"]},{"cell_type":"markdown","source":["## 4.3 Neurons Distribution Analysis"],"metadata":{"id":"T99HBOX8c2gO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOFYYxeJU5nb"},"outputs":[],"source":["k = 25\n","results = []\n","\n","for i in range(1, k):  # i ranges from 1 to k-1\n","    neurons_layer1 = k - i\n","    neurons_layer2 = i\n","\n","    # Create the MLPRegressor model with the specified neurons in two hidden layers\n","    mlp = MLPRegressor(hidden_layer_sizes=(neurons_layer1, neurons_layer2), learning_rate_init=My_Learning_Rate, max_iter=1000, random_state=42)\n","\n","    # Train the model\n","    mlp.fit(X_train, y_train)\n","\n","    # Predict on the testing set\n","    y_pred = mlp.predict(X_test)\n","\n","    # Calculate the mean squared error\n","    mse = mean_squared_error(y_test, y_pred)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    r2 = r2_score(y_test, y_pred)\n","\n","    results.append((neurons_layer1, neurons_layer2, mse, rmse, mae, r2))\n","\n","# Convert results to a DataFrame for better visualization\n","results_df = pd.DataFrame(results, columns=['Neurons in Layer 1', 'Neurons in Layer 2', 'MSE', 'RMSE', 'MAE', 'R²'])\n","\n","# Highlight the best configuration\n","best_config = results_df.loc[results_df['MSE'].idxmin()]\n","print(\"The best configuration is:\")\n","print(f\"Layer 1 neurons: {best_config['Neurons in Layer 1']}, Layer 2 neurons: {best_config['Neurons in Layer 2']}\")\n","print(f\"MSE: {best_config['MSE']}\")\n","print(f\"RMSE: {best_config['RMSE']}\")\n","print(f\"MAE: {best_config['MAE']}\")\n","print(f\"R²: {best_config['R²']}\")\n","\n","# Display the results\n","print(results_df)\n","\n","# Plotting the results\n","plt.figure(figsize=(12, 6))\n","plt.plot(results_df['Neurons in Layer 1'], results_df['MSE'], label='MSE')\n","plt.plot(results_df['Neurons in Layer 1'], results_df['RMSE'], label='RMSE')\n","plt.plot(results_df['Neurons in Layer 1'], results_df['MAE'], label='MAE')\n","plt.xlabel('Neurons in Layer 1')\n","plt.ylabel('Error')\n","plt.title('Error Metrics vs Neurons in Layer 1')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WM8VBH4pU5nd"},"source":["# **Step 5. Long Short-Term Memory (LSTM)**"]},{"cell_type":"markdown","source":["## 5.2 Best Cost Function Analysis"],"metadata":{"id":"cxifTigGdJYQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVu9V8oYUMjF"},"outputs":[],"source":["\n","\n","def create_lstm_datasets(x_scaler, y_scaler, n_steps=N_STEPS):\n","    # Function to create sequences\n","    def create_sequences(data, labels, n_steps):\n","        sequences = []\n","        seq_labels = []\n","        for i in range(len(data) - n_steps):\n","            seq = data[i:i + n_steps]\n","            sequences.append(seq)\n","            seq_labels.append(labels[i + n_steps])\n","        return np.array(sequences), np.array(seq_labels)\n","\n","    # Create sequences\n","    X_sequences, y_sequences = create_sequences(x_scaler, y_scaler, n_steps)\n","\n","    # Calculate split index, keep the same split index as mlp,very important\n","    split_index = int((len(X_sequences)+ n_steps) * 0.7)\n","\n","    # Split the sequences into train and test sets based on time order\n","    X_train_lstm, X_test_lstm = X_sequences[:split_index], X_sequences[split_index:]\n","    y_train_lstm, y_test_lstm = y_sequences[:split_index], y_sequences[split_index:]\n","\n","    # Define input shape\n","    my_input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])\n","\n","    return X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm, my_input_shape\n","\n","\n","# Default Value\n","X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm, my_input_shape = create_lstm_datasets(X_scaler, y_scaler)\n","\n","\n","\n","\n","\n","print(\"time_test shape:\", time_test.shape)\n","print(\"X_train_lstm shape:\", X_train_lstm.shape)\n","print(\"X_test_lstm shape:\", X_test_lstm.shape)\n","print(\"y_train_lstm shape:\", y_train_lstm.shape)\n","print(\"y_test_lstm shape:\", y_test_lstm.shape)\n","print(\"Input shape for LSTM:\", my_input_shape)\n","\n","\n","\n","# Custom callback to measure epoch time\n","class TimeHistory(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.times = []\n","\n","    def on_epoch_begin(self, batch, logs={}):\n","        self.epoch_time_start = time.time()\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.times.append(time.time() - self.epoch_time_start)\n","\n","\n","# Define different loss functions\n","loss_functions = {\n","    'MSE': MeanSquaredError(),\n","    'MAE': MeanAbsoluteError(),\n","    'MSLE': MeanSquaredLogarithmicError(),\n","    'Huber': Huber()\n","}\n","\n","\n","# Build the LSTM model\n","\n","\n","# Function to build LSTM model with stateful mode\n","\n","\n","def build_custom_lstm_model(input_shape_custom\n","                            , learning_rate_custom=0.01\n","                            , loss_function=MeanAbsoluteError()\n","                            , neurons_num=1 ):\n","    model = Sequential()\n","    model.add(LSTM(neurons_num, return_sequences=False, input_shape=input_shape_custom))  # Set return_sequences=False\n","    model.add(Dropout(0.2))\n","    model.add(Dense(neurons_num, activation='relu', kernel_regularizer=l1(0.01)))  # Added L1 regularization\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1))\n","\n","    model.compile(optimizer=Adam(learning_rate=learning_rate_custom), loss=loss_function)\n","\n","    return model\n","\n","\n","\n","def build_custom_lstm_model1(input_shape_custom\n","                            , learning_rate_custom=0.01\n","                            , loss_function=Huber()\n","                            , neurons_num=1 ):\n","    model = Sequential()\n","    model.add(LSTM(neurons_num, return_sequences=True, input_shape=input_shape_custom))\n","    model.add(LSTM(neurons_num))\n","    model.add(Dense(neurons_num, activation='relu', kernel_regularizer=l1(0.01)))  # Added L1 regularization\n","    model.add(Dropout(0.1))\n","    model.add(Dense(neurons_num, activation='relu', kernel_regularizer=l2(0.01)))  # Added L2 regularization\n","    model.add(Dense(1, activation='linear'))\n","\n","    model.compile(optimizer=Adam(learning_rate=learning_rate_custom), loss=loss_function)\n","\n","    return model\n","\n","def build_custom_lstm_model2(input_shape_custom\n","                            , learning_rate_custom=0.01\n","                            , loss_function=MeanSquaredError()\n","                            , neurons_num=1 ):\n","    model = Sequential()\n","    model.add(LSTM(units=neurons_num, return_sequences=True, input_shape=input_shape_custom))\n","    model.add(Dropout(0.2))\n","\n","    model.add(LSTM(units=neurons_num, return_sequences=True))\n","    model.add(Dropout(0.2))\n","\n","    model.add(LSTM(units=neurons_num))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1))\n","\n","    model.compile(optimizer=Adam(learning_rate=learning_rate_custom), loss=loss_function)\n","    return model\n","\n","def build_custom_lstm_model3(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))  # Reduced number of units\n","    model.add(LSTM(64))  # Reduced number of units\n","    model.add(Dense(64, activation='relu', kernel_regularizer=l1(0.01)))  # Added L1 regularization\n","    model.add(Dropout(0.1))\n","    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # Added L2 regularization\n","    model.add(Dense(8, activation='linear'))\n","\n","    model.compile(optimizer='adam', loss='mse')\n","\n","    return model\n","\n","#  select_continuous_sub_data for lstm\n","def select_continuous_sub_data(X_train_lstm, y_train_lstm, X_test_lstm, y_test_lstm):\n","\n","\n","    train_length = 240#2400\n","    test_length = 12#120\n","    #train_length = len(X_train_lstm) // 3\n","    #test_length = len(X_test_lstm) // 3\n","\n","\n","    start_train = 0\n","    start_test = 0\n","    start_train = np.random.randint(0, len(X_train_lstm) - train_length)\n","    start_test = np.random.randint(0, len(X_test_lstm) - test_length)\n","\n","\n","    X_train_segment = X_train_lstm[start_train:start_train + train_length]\n","    y_train_segment = y_train_lstm[start_train:start_train + train_length]\n","    X_test_segment = X_test_lstm[start_test:start_test + test_length]\n","    y_test_segment = y_test_lstm[start_test:start_test + test_length]\n","\n","    return X_train_segment, y_train_segment, X_test_segment, y_test_segment\n","\n","\n","\n","# Inverse transform function\n","def inverse_transform_predictions(predictions, scaler, X_data):\n","    predictions_flat = predictions.reshape(-1, 1)\n","    predictions_actual_flat = scaler.inverse_transform(np.concatenate([np.zeros((predictions_flat.shape[0], X_data.shape[2])), predictions_flat], axis=1))[:, -1]\n","    predictions_actual = predictions_actual_flat.reshape(predictions.shape)\n","    return predictions_actual"]},{"cell_type":"code","source":["# Predict with LSTM\n","#model, lstm_predictions_actual, y_test_lstm_actual = train_lstm(X_train_lstm, y_train_lstm, X_test_lstm, y_test_lstm, best_epoch, best_batch_size, best_neuron_count)\n"],"metadata":{"id":"9OJ0pgJUM8Im"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.3 Best Epoch Analysis"],"metadata":{"id":"4PGaV91Zdaoi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4tClCwiTn1g"},"outputs":[],"source":["MODEL_RUN_COUNT = 10\n","# Function to fit model and calculate Huber loss for each epoch\n","def fit_and_evaluate_epochs(model, X_train_segment, y_train_segment, X_test_segment, y_test_segment, epochs, batch_size, scaler):\n","    train_huber = []\n","    test_huber = []\n","    epoch_times = []\n","\n","    huber_loss = Huber()\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","        history = model.fit(X_train_segment, y_train_segment, epochs=1, batch_size=batch_size, verbose=0)\n","        end_time = time.time()\n","\n","        epoch_time = end_time - start_time\n","        epoch_times.append(epoch_time)\n","\n","        # Predict on train and test sets\n","        y_train_pred = model.predict(X_train_segment, verbose=0)\n","        y_test_pred = model.predict(X_test_segment, verbose=0)\n","\n","        # Inverse transform predictions\n","        y_train_pred_actual = inverse_transform_predictions(y_train_pred, scaler, X_train_segment)\n","        y_test_pred_actual = inverse_transform_predictions(y_test_pred, scaler, X_test_segment)\n","\n","        # Inverse transform actual values\n","        y_train_actual = inverse_transform_predictions(y_train_segment, scaler, X_train_segment)\n","        y_test_actual = inverse_transform_predictions(y_test_segment, scaler, X_test_segment)\n","\n","        # Calculate Huber loss for train and test sets\n","        train_huber.append(np.mean(huber_loss(y_train_actual, y_train_pred_actual).numpy()))\n","        test_huber.append(np.mean(huber_loss(y_test_actual, y_test_pred_actual).numpy()))\n","\n","    return train_huber, test_huber, epoch_times\n","\n","# Function to evaluate model with different parameter configurations\n","def evaluate_model_with_parameters(param_arr, param_name, param_func, fixed_epochs=None, fixed_batch_size=None):\n","    all_test_huber_summary = []\n","    all_test_huber_combined = []\n","    all_times_combined = []\n","\n","    for param in param_arr:\n","        print(f\"Testing with {param_name}: {param}\")\n","\n","        epochs = fixed_epochs if fixed_epochs is not None else param\n","        batch_size = fixed_batch_size if fixed_batch_size is not None else param\n","\n","        all_train_huber = np.zeros((MODEL_RUN_COUNT, epochs))\n","        all_test_huber = np.zeros((MODEL_RUN_COUNT, epochs))\n","        all_epoch_times = np.zeros((MODEL_RUN_COUNT, epochs))\n","\n","        for run in range(MODEL_RUN_COUNT):\n","            model = param_func(param)\n","\n","            # Randomly select continuous sub-data segments\n","            X_train_segment, y_train_segment, X_test_segment, y_test_segment = select_continuous_sub_data(X_train_lstm, y_train_lstm, X_test_lstm, y_test_lstm)\n","\n","            # Fit model and calculate Huber loss for each epoch\n","            train_huber, test_huber, epoch_times = fit_and_evaluate_epochs(model, X_train_segment, y_train_segment, X_test_segment, y_test_segment, epochs, batch_size, my_scaler)\n","            all_train_huber[run] = train_huber\n","            all_test_huber[run] = test_huber\n","            all_epoch_times[run] = epoch_times\n","\n","        all_test_huber_combined.append(all_test_huber.flatten())\n","        all_times_combined.append(np.sum(all_epoch_times, axis=1))\n","\n","        # Calculate summary statistics for test Huber loss\n","        summary_stats = {\n","            param_name: param,\n","            'Test Huber Mean': np.mean(all_test_huber, axis=0)[-1],\n","            'Test Huber Std Dev': np.std(all_test_huber, axis=0)[-1],\n","            'Test Huber Min': np.min(all_test_huber, axis=0)[-1],\n","            'Test Huber Max': np.max(all_test_huber, axis=0)[-1],\n","            'Time Mean': np.mean(np.sum(all_epoch_times, axis=1)),\n","            'Time Std Dev': np.std(np.sum(all_epoch_times, axis=1)),\n","            'Time Min': np.min(np.sum(all_epoch_times, axis=1)),\n","            'Time Max': np.max(np.sum(all_epoch_times, axis=1))\n","        }\n","\n","        all_test_huber_summary.append(summary_stats)\n","\n","        # Plot the Huber loss scores for each epoch\n","        plt.figure(figsize=(14, 7))\n","        mean_train_huber = np.mean(all_train_huber, axis=0)\n","        mean_test_huber = np.mean(all_test_huber, axis=0)\n","\n","        plt.plot(range(1, epochs + 1), mean_train_huber, label='Mean Train Huber', linewidth=2, color='blue')\n","        plt.plot(range(1, epochs + 1), mean_test_huber, label='Mean Test Huber', linewidth=2, color='orange')\n","        plt.fill_between(range(1, epochs + 1), mean_train_huber - np.std(all_train_huber, axis=0), mean_train_huber + np.std(all_train_huber, axis=0), alpha=0.2, color='blue')\n","        plt.fill_between(range(1, epochs + 1), mean_test_huber - np.std(all_test_huber, axis=0), mean_test_huber + np.std(all_test_huber, axis=0), alpha=0.2, color='orange')\n","\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Huber Loss')\n","        plt.title(f'Mean Train and Test Huber Loss Over {epochs} Epochs for {param_name} = {param}')\n","\n","\n","        # Generate x-ticks for 10 evenly spaced points\n","        x_ticks = np.linspace(1, epochs+1, num=10, dtype=int)\n","\n","        # Adjust x-ticks to show only 10 points\n","        plt.xticks(ticks=x_ticks, labels=x_ticks, rotation=45)\n","\n","        plt.legend()\n","        plt.show()\n","\n","    # Combine all summaries into a single DataFrame and display\n","    all_test_huber_summary_df = pd.DataFrame(all_test_huber_summary)\n","    print(f\"Summary of Test Huber Loss and Time for all {param_name} configurations:\\n\", all_test_huber_summary_df)\n","\n","    # Plot the box plot for all test Huber loss\n","    plt.figure(figsize=(14, 7))\n","    plt.boxplot(all_test_huber_combined, labels=[str(param) for param in param_arr])\n","    plt.xlabel(param_name)\n","    plt.ylabel('Test Huber Loss')\n","    plt.title(f'Test Huber Loss Distribution for Different {param_name} Configurations')\n","    plt.show()\n","\n","    # Plot the box plot for all times\n","    plt.figure(figsize=(14, 7))\n","    plt.boxplot(all_times_combined, labels=[str(param) for param in param_arr])\n","    plt.xlabel(param_name)\n","    plt.ylabel('Time (seconds)')\n","    plt.title(f'Time Distribution for Different {param_name} Configurations')\n","    plt.show()\n","\n","# Main logic\n","if CALCU_HYPER_PARAMETERS:\n","    # Example usage to find best epochs\n","    epochs_arr = [20, 60,100,200,500]  # Example epochs to test\n","    evaluate_model_with_parameters(epochs_arr, 'Epochs', lambda x: build_custom_lstm_model(my_input_shape))\n"]},{"cell_type":"markdown","source":["## 5.4 Batch Size Analysis"],"metadata":{"id":"x_PMxcErdjEe"}},{"cell_type":"code","source":["\n","# Main logic\n","if CALCU_HYPER_PARAMETERS:\n","  # Example usage to find best batch size\n","    batch_sizes = [4, 8, 16, 32, 64,128,256]  # Example batch sizes to test\n","    evaluate_model_with_parameters(batch_sizes, 'Batch Size', lambda x: build_custom_lstm_model(my_input_shape), fixed_epochs=200)\n"],"metadata":{"id":"1599Qg8bLtvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.5 Number of Neurons in Hidden Layer Analysis"],"metadata":{"id":"ZojT_DxldsEb"}},{"cell_type":"code","source":["\n","# Main logic\n","if CALCU_HYPER_PARAMETERS:\n","    # Example usage to find best neuron count\n","    neuron_counts = [1, 2, 4, 8,16,32,64,128,256]  # Example neuron counts to test\n","    evaluate_model_with_parameters(neuron_counts, 'Neuron Count', lambda x: build_custom_lstm_model(input_shape_custom=my_input_shape, neurons_num=x), fixed_epochs=200, fixed_batch_size=256)\n"],"metadata":{"id":"uVOZjJN8LwqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 6. Model Comparison**"],"metadata":{"id":"btM4-XObd6e7"}},{"cell_type":"code","source":["\n","# Define the best hyperparameters\n","\n","# LSTM Replace with the best hyperparameters found\n","best_epoch = 200\n","best_batch_size = 64\n","best_neuron_count = 8\n","\n","# MLP Replace with the best hyperparameters found\n","neurons_layer1 = 15\n","neurons_layer2 = 10\n","learning_rate = 0.01\n","\n","# Create and train the MLP model\n","def train_mlp(X_train, y_train, X_test, y_test, neurons_layer1, neurons_layer2, learning_rate):\n","    mlp = MLPRegressor(hidden_layer_sizes=(neurons_layer1, neurons_layer2), learning_rate_init=learning_rate, max_iter=1000, random_state=42)\n","    mlp.fit(X_train, y_train)\n","    mlp_predictions = mlp.predict(X_test)\n","    mlp_predictions_actual = inverse_transform_predictions(mlp_predictions, my_scaler, X_test)\n","    y_test_actual = inverse_transform_predictions(y_test, my_scaler, X_test)\n","    return mlp, mlp_predictions_actual, y_test_actual\n","\n","# Create and train the LSTM model\n","def train_lstm(X_train, y_train, X_test, y_test, epochs, batch_size, neuron_count):\n","    model = build_custom_lstm_model(input_shape_custom=my_input_shape, neurons_num=neuron_count)\n","    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)\n","    lstm_predictions = model.predict(X_test)\n","    print(\"Shape of X_train:\", X_train.shape)\n","    print(\"Shape of y_train:\", y_train.shape)\n","    print(\"Shape of X_test:\", X_test.shape)\n","    print(\"Shape of lstm_predictions:\", lstm_predictions.shape)\n","    print(\"Shape of lstm_predictions:\", lstm_predictions)\n","    lstm_predictions_actual = inverse_transform_predictions(lstm_predictions, my_scaler, X_test)\n","    y_test_actual = inverse_transform_predictions(y_test, my_scaler, X_test)\n","    return model, lstm_predictions_actual, y_test_actual\n","\n","# Function to calculate performance metrics\n","def calculate_metrics(actual, predicted):\n","    mse = mean_squared_error(actual, predicted)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(actual, predicted)\n","    r2 = r2_score(actual, predicted)\n","    return mse, rmse, mae, r2\n","\n","# Function to plot performance metrics\n","def plot_metrics(y_test_actual, mlp_predictions_actual, lstm_predictions_actual, time_test):\n","    plt.figure(figsize=(14, 7))\n","    plt.plot(time_test, y_test_actual, label='Actual PM2.5', color='blue')\n","    plt.plot(time_test, mlp_predictions_actual, label='Predicted PM2.5 (MLP)', color='orange', alpha=0.7)\n","    plt.plot(time_test, lstm_predictions_actual, label='Predicted PM2.5 (LSTM)', color='green', alpha=0.7)\n","    plt.title('PM2.5 Real and Prediction Values in MLP and LSTM')\n","    plt.xlabel('Time')\n","    plt.ylabel('PM2.5 (µg/m³)')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Function to plot comparison bar charts\n","def plot_comparison_bar_chart(mlp_metrics, lstm_metrics):\n","    labels = ['RMSE', 'MAE', 'R²']\n","    x = np.arange(len(labels))  # Number of metrics\n","    width = 0.35  # Width of the bars\n","    fig, ax = plt.subplots(figsize=(10, 6))\n","    rects1 = ax.bar(x - width/2, mlp_metrics, width, label='MLP')\n","    rects2 = ax.bar(x + width/2, lstm_metrics, width, label='LSTM')\n","    ax.set_xlabel('Metrics')\n","    ax.set_ylabel('Scores')\n","    ax.set_title('Performance Comparison of MLP and LSTM')\n","    ax.set_xticks(x)\n","    ax.set_xticklabels(labels)\n","    ax.legend()\n","    def autolabel(rects):\n","        for rect in rects:\n","            height = rect.get_height()\n","            ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n","    autolabel(rects1)\n","    autolabel(rects2)\n","    fig.tight_layout()\n","    plt.show()\n","\n","# Inverse transform function\n","def inverse_transform_predictions(predictions, scaler, X_data):\n","    predictions_flat = predictions.reshape(-1, 1)\n","    zeros_shape = (predictions_flat.shape[0], X_data.shape[1]) if X_data.ndim == 2 else (predictions_flat.shape[0], X_data.shape[2])\n","    predictions_actual_flat = scaler.inverse_transform(np.concatenate([np.zeros(zeros_shape), predictions_flat], axis=1))[:, -1]\n","    predictions_actual = predictions_actual_flat.reshape(predictions.shape)\n","    return predictions_actual\n","\n","train_size = int(len(X_scaler) * 0.7)\n","X_train, X_test = X_scaler[:train_size], X_scaler[train_size:]\n","y_train, y_test = y_scaler[:train_size], y_scaler[train_size:]\n","\n","time_test = df.index[train_size + N_STEPS:]\n","\n","mlp, mlp_predictions_actual, y_test_actual = train_mlp(X_train, y_train, X_test, y_test, neurons_layer1, neurons_layer2, learning_rate)\n","model, lstm_predictions_actual, y_test_lstm_actual = train_lstm(X_train_lstm, y_train_lstm, X_test_lstm, y_test_lstm, best_epoch, best_batch_size, best_neuron_count)\n"],"metadata":{"id":"nzOiK-FXDInk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","lstm_predictions_actual = lstm_predictions_actual.reshape(-1)\n","\n","# Ensure shapes are consistent\n","print('Shape of y_test_actual:', y_test_actual.shape)\n","print('Shape of mlp_predictions_actual:', mlp_predictions_actual.shape)\n","print('Shape of y_test_lstm_actual:', y_test_lstm_actual.shape)\n","print('Shape of lstm_predictions_actual:', lstm_predictions_actual.shape)\n","\n"],"metadata":{"id":"Qvfrd8B2XyLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm_predictions_actual"],"metadata":{"id":"b8tzh0oakU1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Calculate metrics\n","mlp_mse, mlp_rmse, mlp_mae, mlp_r2 = calculate_metrics(y_test_actual, mlp_predictions_actual)\n","lstm_mse, lstm_rmse, lstm_mae, lstm_r2 = calculate_metrics(y_test_lstm_actual, lstm_predictions_actual)\n","\n","# Plot actual and predicted PM2.5 values\n","plot_metrics(y_test_actual[N_STEPS:], mlp_predictions_actual[N_STEPS:], lstm_predictions_actual, time_test)\n","\n","# Plot performance metrics comparison\n","plot_comparison_bar_chart([mlp_rmse, mlp_mae, mlp_r2], [lstm_rmse, lstm_mae, lstm_r2])\n","\n","# Print performance metrics for MLP\n","print(\"MLP Performance:\")\n","print(f\"RMSE: {mlp_rmse}\")\n","print(f\"MAE: {mlp_mae}\")\n","print(f\"R²: {mlp_r2}\")\n","\n","# Print performance metrics for LSTM\n","print(\"\\nLSTM Performance:\")\n","print(f\"RMSE: {lstm_rmse}\")\n","print(f\"MAE: {lstm_mae}\")\n","print(f\"R²: {lstm_r2}\")\n","\n","# Print model summaries\n","print(\"\\nMLP Model Summary:\")\n","print(f\"Hidden Layers: {mlp.hidden_layer_sizes}\")\n","print(f\"Number of iterations: {mlp.n_iter_}\")\n","print(f\"Learning rate: {mlp.learning_rate_init}\")\n","\n","print(\"\\nLSTM Model Summary:\")\n","model.summary()"],"metadata":{"id":"EsvWPGLKY2yC"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}